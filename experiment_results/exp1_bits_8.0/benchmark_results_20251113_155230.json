{
  "squad": [
    {
      "id": "56ddde6b9a695914005b9628",
      "dataset": "squad",
      "question": "In what country is Normandy located?",
      "prediction": "Normandy is located in France.",
      "ground_truth": "France",
      "is_answerable": true,
      "contextual_relevance": 0.9297821521759033,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.5070251226425171,
      "response_coherence": 1.0,
      "communication_efficiency": 1.0,
      "information_throughput": 0.18960379295826207,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.274156093597412,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56ddde6b9a695914005b9629",
      "dataset": "squad",
      "question": "When were the Normans in Normandy?",
      "prediction": "Question: When were the Normans in Normandy?\n\nAnswer: The Normans in Normandy were first mentioned in the 10th and 11th centuries.",
      "ground_truth": "10th and 11th centuries",
      "is_answerable": true,
      "contextual_relevance": 0.7684991359710693,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.4457821249961853,
      "response_coherence": 0.9142857142857143,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1410931735128843,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.087515115737915,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56ddde6b9a695914005b962a",
      "dataset": "squad",
      "question": "From which countries did the Norse originate?",
      "prediction": "The Norse originated from the Scandinavian countries of Denmark, Iceland, and Norway.",
      "ground_truth": "Denmark, Iceland and Norway",
      "is_answerable": true,
      "contextual_relevance": 0.9019361734390259,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.6511962413787842,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 0.5,
      "information_throughput": 0.07723779081308889,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.473515033721924,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56ddde6b9a695914005b962b",
      "dataset": "squad",
      "question": "Who was the Norse leader?",
      "prediction": "The Norse leader referred to in the context is Rollo.",
      "ground_truth": "Rollo",
      "is_answerable": true,
      "contextual_relevance": 0.7164544463157654,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.5444837808609009,
      "response_coherence": 0.97,
      "communication_efficiency": 1.0,
      "information_throughput": 0.17269179700541848,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.790663003921509,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56ddde6b9a695914005b962c",
      "dataset": "squad",
      "question": "What century did the Normans first gain their separate identity?",
      "prediction": "Question: What century did the Normans first gain their separate identity?\n\nAnswer: The Normans first gained their separate identity in the first half of the 10th century.",
      "ground_truth": "10th century",
      "is_answerable": true,
      "contextual_relevance": 0.8503868579864502,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.42459502816200256,
      "response_coherence": 0.9111111111111111,
      "communication_efficiency": 1.0,
      "information_throughput": 0.13138548576904494,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.6111907958984375,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad39d53604f3c001a3fe8d1",
      "dataset": "squad",
      "question": "Who gave their name to Normandy in the 1000's and 1100's",
      "prediction": "The context given in the question is the history of the Normans, a group of people who gave their name to Normandy in the 10th and 11th centuries. The Normans were descended from Norse raiders and",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.6468772888183594,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.09875074028968811,
      "response_coherence": 0.9333333333333333,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.613187789916992,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad39d53604f3c001a3fe8d2",
      "dataset": "squad",
      "question": "What is France a region of?",
      "prediction": "The given context mentions that France is a region in France.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.8845317363739014,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.09920412302017212,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.124678134918213,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad39d53604f3c001a3fe8d3",
      "dataset": "squad",
      "question": "Who did King Charles III swear fealty to?",
      "prediction": "The question asks who the leader of the Normans, the descendants of Norse raiders and pirates from Denmark, Iceland, and Norway, agreed to swear fealty to. The answer is King Charles III of West Francia",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.6052124500274658,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.17026065289974213,
      "response_coherence": 0.94,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.092013120651245,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad39d53604f3c001a3fe8d4",
      "dataset": "squad",
      "question": "When did the Frankish identity emerge?",
      "prediction": "Question: When did the distinct cultural and ethnic identity of the Normans emerge?",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.5298067331314087,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.11935856938362122,
      "response_coherence": 0.9769230769230769,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.490714073181152,
      "compression_ratio": 5.062738630635399,
      "layers_transmitted": 15,
      "bits_original": 45711360,
      "bits_compressed": 9910368,
      "bits_saved": 35800992,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dddf4066d3e219004dad5f",
      "dataset": "squad",
      "question": "Who was the duke in the battle of Hastings?",
      "prediction": "The duke in the battle of Hastings was William the Conqueror.",
      "ground_truth": "William the Conqueror",
      "is_answerable": true,
      "contextual_relevance": 0.842660665512085,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.7485625743865967,
      "response_coherence": 0.9454545454545454,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1129943241035551,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.85000205039978,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dddf4066d3e219004dad60",
      "dataset": "squad",
      "question": "Who ruled the duchy of Normandy",
      "prediction": "Key Information: 1066; medieval Europe; major political; Norman dynasty; Norman; Norman French; military impact; Norman adventurers founded; Europe; Norman cultural; Christian piety; Duchy of Normandy; Catholic orth",
      "ground_truth": "Richard I",
      "is_answerable": true,
      "contextual_relevance": 0.47434142231941223,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1523735374212265,
      "response_coherence": 0.7555555555555555,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 11.097336053848267,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dddf4066d3e219004dad61",
      "dataset": "squad",
      "question": "What religion were the Normans",
      "prediction": "The context is the Norman dynasty's major political, cultural, and military impact on medieval Europe and the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic",
      "ground_truth": "Catholic",
      "is_answerable": true,
      "contextual_relevance": 0.7194982767105103,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.3636970520019531,
      "response_coherence": 0.9368421052631578,
      "communication_efficiency": 1.0,
      "information_throughput": 0.09302317122636794,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 10.750009775161743,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3a266604f3c001a3fea27",
      "dataset": "squad",
      "question": "What type of major impact did the Norman dynasty have on modern Europe?",
      "prediction": "The Norman dynasty had a major political, cultural, and military impact on modern Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodox",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7810266017913818,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.06982719898223877,
      "response_coherence": 0.9461538461538461,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 11.201196908950806,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3a266604f3c001a3fea28",
      "dataset": "squad",
      "question": "Who was famed for their Christian spirit?",
      "prediction": "The Norman dynasty was famed for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.4203779101371765,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1167176216840744,
      "response_coherence": 0.9842105263157894,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.688200950622559,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3a266604f3c001a3fea29",
      "dataset": "squad",
      "question": "Who assimilted the Roman language?",
      "prediction": "The Norman dynasty assimilated the Roman language, which they adopted into their own dialect, known as Norman, Normaund or Norman French.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.5296210646629333,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.08306331932544708,
      "response_coherence": 0.9714285714285713,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.85791015625,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3a266604f3c001a3fea2a",
      "dataset": "squad",
      "question": "Who ruled the country of Normandy?",
      "prediction": "The question asks about the ruling dynasty of Normandy, which is the Duchy of Normandy. The answer is that the ruling dynasty was the Norman dynasty, which was founded by the Normans who settled in Normandy,",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.69512939453125,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0952424630522728,
      "response_coherence": 0.8916666666666666,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 10.998560905456543,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3a266604f3c001a3fea2b",
      "dataset": "squad",
      "question": "What principality did William the conquerer found?",
      "prediction": "The question asks about the principality founded by William the Conqueror, who conquered southern Italy on the Saracens and Byzantines and founded the Kingdom of Sicily.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.5899067521095276,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.197445809841156,
      "response_coherence": 0.9307692307692308,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 10.272250890731812,
      "compression_ratio": 5.064389776523531,
      "layers_transmitted": 15,
      "bits_original": 78888960,
      "bits_compressed": 17098848,
      "bits_saved": 61790112,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde0379a695914005b9636",
      "dataset": "squad",
      "question": "What is the original meaning of the word Norman?",
      "prediction": "The original meaning of the word Norman is \"Northman\" or \"Viking\" in Old Low Franconian Nortmann, Latinized as Normannus, and recorded in Medieval Latin as Nortmannus. The word",
      "ground_truth": "Viking",
      "is_answerable": true,
      "contextual_relevance": 0.7519041895866394,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.4494130313396454,
      "response_coherence": 0.9464285714285714,
      "communication_efficiency": 1.0,
      "information_throughput": 0.17088107457746055,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.852023124694824,
      "compression_ratio": 5.05964633353936,
      "layers_transmitted": 15,
      "bits_original": 25559040,
      "bits_compressed": 5544032,
      "bits_saved": 20015008,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde0379a695914005b9637",
      "dataset": "squad",
      "question": "When was the Latin version of the word Norman first recorded?",
      "prediction": "The Latin version of the word Norman was first recorded in Medieval Latin, which is the 9th century.",
      "ground_truth": "9th century",
      "is_answerable": true,
      "contextual_relevance": 0.8433653116226196,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.5355473160743713,
      "response_coherence": 0.9666666666666666,
      "communication_efficiency": 1.0,
      "information_throughput": 0.22090323069942616,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 4.526869058609009,
      "compression_ratio": 5.05964633353936,
      "layers_transmitted": 15,
      "bits_original": 25559040,
      "bits_compressed": 5544032,
      "bits_saved": 20015008,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ab70604f3c001a3feb89",
      "dataset": "squad",
      "question": "What name comes from the English words Normans/Normanz?",
      "prediction": "The answer is \"Normans\".",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7875663638114929,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.22100991010665894,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 3.817359685897827,
      "compression_ratio": 5.05964633353936,
      "layers_transmitted": 15,
      "bits_original": 25559040,
      "bits_compressed": 5544032,
      "bits_saved": 20015008,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ab70604f3c001a3feb8a",
      "dataset": "squad",
      "question": "When was the French version of the word Norman first recorded?",
      "prediction": "The question asks when the French version of the word Norman first became recorded in Medieval Latin. The answer is that the French version of the word Norman was recorded in Medieval Latin as Normanus, which was borrowed from Old Low Franconian",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7834414839744568,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.07666359841823578,
      "response_coherence": 0.8999999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.038495302200317,
      "compression_ratio": 5.05964633353936,
      "layers_transmitted": 15,
      "bits_original": 25559040,
      "bits_compressed": 5544032,
      "bits_saved": 20015008,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde0ba66d3e219004dad75",
      "dataset": "squad",
      "question": "When was the Duchy of Normandy founded?",
      "prediction": "Question: When was the Duchy of Normandy founded?\n\nAnswer: The Duchy of Normandy was founded in 911 as a fiefdom under the treaty of Saint-Clair-sur-Epte between",
      "ground_truth": "911",
      "is_answerable": true,
      "contextual_relevance": 0.8229227066040039,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.22452837228775024,
      "response_coherence": 0.9192307692307692,
      "communication_efficiency": 1.0,
      "information_throughput": 0.11303701187161036,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.846659898757935,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde0ba66d3e219004dad76",
      "dataset": "squad",
      "question": "Who did Rollo sign the treaty of Saint-Clair-sur-Epte with?",
      "prediction": "King Charles III of West Francia and the famed Viking ruler Rollo signed the treaty of Saint-Clair-sur-Epte in exchange for the French lands between the river Epte and the Atlantic coast.",
      "ground_truth": "King Charles III",
      "is_answerable": true,
      "contextual_relevance": 0.7869126796722412,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.4070168137550354,
      "response_coherence": 0.9419354838709677,
      "communication_efficiency": 1.0,
      "information_throughput": 0.11092921343426261,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.014757871627808,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde0ba66d3e219004dad77",
      "dataset": "squad",
      "question": "What river originally bounded the Duchy",
      "prediction": "Key Information: 10 th; 911 as; Norse war bands; included local women; initially destructive incursions; King Charles III; Norse war; France evolved; Viking ruler Rollo; France",
      "ground_truth": "Seine",
      "is_answerable": true,
      "contextual_relevance": 0.1732201874256134,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.11997784674167633,
      "response_coherence": 0.7769230769230769,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.01422905921936,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ad61604f3c001a3fec0d",
      "dataset": "squad",
      "question": "when did Nors encampments ivolve into destructive incursions?",
      "prediction": "Question: When did Norse encampments involving local women and personal property evolve into more permanent incursions?",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.3957281708717346,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.08591356873512268,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.433876991271973,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ad61604f3c001a3fec0e",
      "dataset": "squad",
      "question": "What treaty was established in the 9th century?",
      "prediction": "The question asks about the treaty established in the 9th century between King Charles III of West Francia and the famed Viking ruler Rollo.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.597568154335022,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1955340951681137,
      "response_coherence": 0.9624999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.030595779418945,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ad61604f3c001a3fec0f",
      "dataset": "squad",
      "question": "Who established a treaty with King Charles the third of France?",
      "prediction": "The question asks who established a treaty with King Charles III of West Francia and the famed Viking ruler Rollo in the 10th century. The answer is that the treaty was established by King Charles III of West Francia and",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.543041467666626,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.18923646211624146,
      "response_coherence": 0.9025,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.884758949279785,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ad61604f3c001a3fec10",
      "dataset": "squad",
      "question": "What did the French promises to protect Rollo and his men from?",
      "prediction": "The French promises to protect Rollo and his men from further Viking incursions were offered in exchange for their protection against further Viking incursions.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7628508806228638,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0194386038929224,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.819803953170776,
      "compression_ratio": 5.063389895247934,
      "layers_transmitted": 15,
      "bits_original": 54804480,
      "bits_compressed": 11880544,
      "bits_saved": 42923936,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde1d966d3e219004dad8d",
      "dataset": "squad",
      "question": "Who upon arriving gave the original viking settlers a common identity?",
      "prediction": "The question asks who upon arriving gave the original viking settlers a common identity. The given context does not provide any information on who specifically gave the original viking settlers a common identity.",
      "ground_truth": "Rollo",
      "is_answerable": true,
      "contextual_relevance": 0.8503237366676331,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.018799863755702972,
      "response_coherence": 0.8999999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.574383974075317,
      "compression_ratio": 5.062443735964515,
      "layers_transmitted": 15,
      "bits_original": 42516480,
      "bits_compressed": 9218144,
      "bits_saved": 33298336,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ae14604f3c001a3fec39",
      "dataset": "squad",
      "question": "When did Rollo begin to arrive in Normandy?",
      "prediction": "Question: When did Rollo begin to arrive in Normandy?\n\nAnswer: Rollo began to arrive in Normandy in the 880s, after earlier Viking settlers had begun arriving in the 880s",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7820476293563843,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.07931169122457504,
      "response_coherence": 0.9275862068965517,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.226404905319214,
      "compression_ratio": 5.062443735964515,
      "layers_transmitted": 15,
      "bits_original": 42516480,
      "bits_compressed": 9218144,
      "bits_saved": 33298336,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3ae14604f3c001a3fec3a",
      "dataset": "squad",
      "question": "What Viking groups were conquered by Rollo?",
      "prediction": "The given context mentions that Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes,",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.750375509262085,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.053788237273693085,
      "response_coherence": 0.7777777777777778,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.142486095428467,
      "compression_ratio": 5.062443735964515,
      "layers_transmitted": 15,
      "bits_original": 42516480,
      "bits_compressed": 9218144,
      "bits_saved": 33298336,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde27d9a695914005b9651",
      "dataset": "squad",
      "question": "What was the Norman religion?",
      "prediction": "The Norman religion was Catholicism, which replaced the Norse religion and Old Norse language with Christianity.",
      "ground_truth": "Catholicism",
      "is_answerable": true,
      "contextual_relevance": 0.7581783533096313,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.4535462260246277,
      "response_coherence": 0.94375,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1908328435463363,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.240188121795654,
      "compression_ratio": 5.061216155520211,
      "layers_transmitted": 15,
      "bits_original": 32931840,
      "bits_compressed": 7141472,
      "bits_saved": 25790368,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde27d9a695914005b9652",
      "dataset": "squad",
      "question": "What part of France were the Normans located?",
      "prediction": "The context states that the descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the",
      "ground_truth": "north",
      "is_answerable": true,
      "contextual_relevance": 0.25449851155281067,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.06776076555252075,
      "response_coherence": 0.7129032258064516,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.061185121536255,
      "compression_ratio": 5.061216155520211,
      "layers_transmitted": 15,
      "bits_original": 32931840,
      "bits_compressed": 7141472,
      "bits_saved": 25790368,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3af11604f3c001a3fec63",
      "dataset": "squad",
      "question": "What was replace with the Norse religion?",
      "prediction": "The Norse religion was replaced with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7802055478096008,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.04522513598203659,
      "response_coherence": 0.7482758620689656,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.020723104476929,
      "compression_ratio": 5.061216155520211,
      "layers_transmitted": 15,
      "bits_original": 32931840,
      "bits_compressed": 7141472,
      "bits_saved": 25790368,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3af11604f3c001a3fec64",
      "dataset": "squad",
      "question": "What did maternal Old Norse traditions merge with?",
      "prediction": "The maternal Old Norse traditions of the descendants of Rollo's Vikings and their Frankish wives would merge with Catholicism and the Gallo-Romance language of the local people to synthesize a unique \"",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7360100150108337,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.04667286574840546,
      "response_coherence": 0.74375,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.139745712280273,
      "compression_ratio": 5.061216155520211,
      "layers_transmitted": 15,
      "bits_original": 32931840,
      "bits_compressed": 7141472,
      "bits_saved": 25790368,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3af11604f3c001a3fec65",
      "dataset": "squad",
      "question": "What language replaced the Gallo-Romance language?",
      "prediction": "The question asks about the language that replaced the Gallo-Romance language in the north of France. The context states that the descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.6746170520782471,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.14781010150909424,
      "response_coherence": 0.9166666666666666,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.091209888458252,
      "compression_ratio": 5.061216155520211,
      "layers_transmitted": 15,
      "bits_original": 32931840,
      "bits_compressed": 7141472,
      "bits_saved": 25790368,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56dde2fa66d3e219004dad9b",
      "dataset": "squad",
      "question": "What was one of the Norman's major exports?",
      "prediction": "One of the Norman's major exports was fighting horsemen.",
      "ground_truth": "fighting horsemen",
      "is_answerable": true,
      "contextual_relevance": 0.7803384065628052,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.6240970492362976,
      "response_coherence": 1.0,
      "communication_efficiency": 1.0,
      "information_throughput": 0.18575399362069409,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.383464336395264,
      "compression_ratio": 5.0621292992043285,
      "layers_transmitted": 15,
      "bits_original": 39567360,
      "bits_compressed": 8579168,
      "bits_saved": 30988192,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3c626604f3c001a3ff011",
      "dataset": "squad",
      "question": "Who adopted the fuedel doctrines of the Normans?",
      "prediction": "The context states that the Normans adopted the growing feudal doctrines of the rest of France, which were culturally and ethnically distinct from the old French aristocracy. The new Norman rulers were culturally and",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.6490105390548706,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.02230912819504738,
      "response_coherence": 0.9205882352941176,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.529839754104614,
      "compression_ratio": 5.0621292992043285,
      "layers_transmitted": 15,
      "bits_original": 39567360,
      "bits_compressed": 8579168,
      "bits_saved": 30988192,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3c626604f3c001a3ff012",
      "dataset": "squad",
      "question": "What was one of the Norman's major imports?",
      "prediction": "One of the Norman's major imports was fighting horsemen.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7783569097518921,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.04642891138792038,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.24580192565918,
      "compression_ratio": 5.0621292992043285,
      "layers_transmitted": 15,
      "bits_original": 39567360,
      "bits_compressed": 8579168,
      "bits_saved": 30988192,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3c626604f3c001a3ff013",
      "dataset": "squad",
      "question": "Who's arristocracy eventually served as avid Crusaders?",
      "prediction": "The question asks who served as avid Crusaders under the Italo-Norman prince Bohemund I and the Anglo-Norman king Richard the Lion-Heart. The answer is that the Norman aristocracy, which",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.6155861616134644,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.16699351370334625,
      "response_coherence": 0.9482758620689655,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.411940813064575,
      "compression_ratio": 5.0621292992043285,
      "layers_transmitted": 15,
      "bits_original": 39567360,
      "bits_compressed": 8579168,
      "bits_saved": 30988192,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56de0f6a4396321400ee257f",
      "dataset": "squad",
      "question": "Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?",
      "prediction": "The Normans' main enemy in Italy was the Byzantine Empire and Armenia.",
      "ground_truth": "Seljuk Turks",
      "is_answerable": true,
      "contextual_relevance": 0.9594048261642456,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.2210269272327423,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.759875297546387,
      "compression_ratio": 5.062491953471449,
      "layers_transmitted": 15,
      "bits_original": 43008000,
      "bits_compressed": 9324640,
      "bits_saved": 33683360,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3dbc6604f3c001a3ff3e9",
      "dataset": "squad",
      "question": "Who entered Italy soon after the Byzantine Empire?",
      "prediction": "The given context mentions that the Normans began to enter Italy soon after the Byzantine Empire.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7035114765167236,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.06416381895542145,
      "response_coherence": 0.9624999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.771393060684204,
      "compression_ratio": 5.062491953471449,
      "layers_transmitted": 15,
      "bits_original": 43008000,
      "bits_compressed": 9324640,
      "bits_saved": 33683360,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3dbc6604f3c001a3ff3ea",
      "dataset": "squad",
      "question": "Who did the Normans fight in Italy?",
      "prediction": "The Normans fought in Italy, specifically in the Byzantine Empire and Armenia, against the Pechenegs, the Bulgars, and the Seljuk Turks.\n\nContext: The Normans began to enter Italy in the",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7631350755691528,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.015779729932546616,
      "response_coherence": 0.8999999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.5699849128723145,
      "compression_ratio": 5.062491953471449,
      "layers_transmitted": 15,
      "bits_original": 43008000,
      "bits_compressed": 9324640,
      "bits_saved": 33683360,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3dbc6604f3c001a3ff3eb",
      "dataset": "squad",
      "question": "Who did the Normans encourage to come to the south?",
      "prediction": "The question asks who the Normans encouraged to come to the south, specifically in relation to the Norman mercenaries fighting in Byzantine service against the Byzantines. The answer is that the Normans encouraged the Lombards to",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.7220183610916138,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.19589023292064667,
      "response_coherence": 0.8916666666666666,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.483393907546997,
      "compression_ratio": 5.062491953471449,
      "layers_transmitted": 15,
      "bits_original": 43008000,
      "bits_compressed": 9324640,
      "bits_saved": 33683360,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3dbc6604f3c001a3ff3ec",
      "dataset": "squad",
      "question": "During what campaign did the Vargian and Lombard fight?",
      "prediction": "During the Sicilian campaign of George Maniaces in 1038\u201340, the Norman mercenaries were primarily involved in Byzantine service in Sicily.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.40981006622314453,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0859188660979271,
      "response_coherence": 0.955,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.090464115142822,
      "compression_ratio": 5.062491953471449,
      "layers_transmitted": 15,
      "bits_original": 43008000,
      "bits_compressed": 9324640,
      "bits_saved": 33683360,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56de0ffd4396321400ee258d",
      "dataset": "squad",
      "question": "When did Herve serve as a Byzantine general?",
      "prediction": "Question: When did Herv\u00e9 serve as a Byzantine general?\n\nAnswer: Herv\u00e9 served as a Byzantine general in the 1050s.",
      "ground_truth": "1050s",
      "is_answerable": true,
      "contextual_relevance": 0.894086480140686,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.13896554708480835,
      "response_coherence": 0.9368421052631578,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1590541476790737,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.287167072296143,
      "compression_ratio": 5.061663647088471,
      "layers_transmitted": 15,
      "bits_original": 35880960,
      "bits_compressed": 7780448,
      "bits_saved": 28100512,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56de0ffd4396321400ee258e",
      "dataset": "squad",
      "question": "When did Robert Crispin go up against the Turks?",
      "prediction": "Question: When did Robert Crispin lead the Normans of Edessa against the Turks?",
      "ground_truth": "1060s",
      "is_answerable": true,
      "contextual_relevance": 0.8241344690322876,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.11102843284606934,
      "response_coherence": 0.9769230769230769,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.412830829620361,
      "compression_ratio": 5.061663647088471,
      "layers_transmitted": 15,
      "bits_original": 35880960,
      "bits_compressed": 7780448,
      "bits_saved": 28100512,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "56de0ffd4396321400ee258f",
      "dataset": "squad",
      "question": "Who ruined Roussel de Bailleul's plans for an independent state?",
      "prediction": "The question asks who ruined Roussel de Bailleul's plans for an independent state. The answer is that it was the Byzantine general Alexius Komnenos, who stopped Roussel de Bailleul from carving",
      "ground_truth": "Alexius Komnenos",
      "is_answerable": true,
      "contextual_relevance": 0.7722028493881226,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.31668174266815186,
      "response_coherence": 0.9516129032258064,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1379145438989568,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.250866889953613,
      "compression_ratio": 5.061663647088471,
      "layers_transmitted": 15,
      "bits_original": 35880960,
      "bits_compressed": 7780448,
      "bits_saved": 28100512,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3de8b604f3c001a3ff467",
      "dataset": "squad",
      "question": "Who was the first Byzantine mercenary to serve with the Normans?",
      "prediction": "The first Byzantine mercenary to serve with the Normans was Herv\u00e9 in the 1050s.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.8522602319717407,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.10240881145000458,
      "response_coherence": 0.9571428571428571,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 5.9254701137542725,
      "compression_ratio": 5.061663647088471,
      "layers_transmitted": 15,
      "bits_original": 35880960,
      "bits_compressed": 7780448,
      "bits_saved": 28100512,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ad3de8b604f3c001a3ff468",
      "dataset": "squad",
      "question": "When did Herve serve as a Norman general?",
      "prediction": "Question: When did Herv\u00e9 serve as a Norman general?\n\nAnswer: Herv\u00e9 served as a Norman general in the 1050s.",
      "ground_truth": "unanswerable",
      "is_answerable": false,
      "contextual_relevance": 0.8875471353530884,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1778547763824463,
      "response_coherence": 0.9368421052631578,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.227519989013672,
      "compression_ratio": 5.061663647088471,
      "layers_transmitted": 15,
      "bits_original": 35880960,
      "bits_compressed": 7780448,
      "bits_saved": 28100512,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    }
  ],
  "hotpot_qa": [
    {
      "id": "5a8b57f25542995d1e6f1371",
      "dataset": "hotpot_qa",
      "question": "Were Scott Derrickson and Ed Wood of the same nationality?",
      "prediction": "genres.  He has composed music for films such as \"The Conjuring\" (2013), \"The Conjuring 2\" (2016), \"The Nun\" (2018), \"",
      "ground_truth": "yes",
      "is_answerable": true,
      "contextual_relevance": 0.0,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.056785646826028824,
      "response_coherence": 0.77,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 14.021328926086426,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8c7595554299585d9e36b6",
      "dataset": "hotpot_qa",
      "question": "What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?",
      "prediction": "series \"Meet Corliss Archer\" and was produced by Ziv Television.  The show starred Shirley Temple as Corliss Archer, a young woman who is the daughter of a wealthy businessman.  The",
      "ground_truth": "Chief of Protocol",
      "is_answerable": true,
      "contextual_relevance": 0.5801207423210144,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.09248116612434387,
      "response_coherence": 0.76,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.024233819743181614,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.754882097244263,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a85ea095542994775f606a8",
      "dataset": "hotpot_qa",
      "question": "What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?",
      "prediction": "was published by the British Science Fiction Association, and was edited by John Carnell.  The magazine was published by the British Science Fiction Association from 1950 to 1964, and by Roberts & Vinter from",
      "ground_truth": "Animorphs",
      "is_answerable": true,
      "contextual_relevance": 0.2404697835445404,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.6852941176470588,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.644299983978271,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5adbf0a255429947ff17385a",
      "dataset": "hotpot_qa",
      "question": "Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?",
      "prediction": "\u0645\u0633\u062c\u062f \u0627\u0644\u062c\u0632\u064a\u0631\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629, romanized: Masjid al-Jazira al-\u02bfArabiyya) is a mosque located in the Algiers district of Al",
      "ground_truth": "no",
      "is_answerable": true,
      "contextual_relevance": 0.47225433588027954,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.6,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.63702392578125,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8e3ea95542995a26add48d",
      "dataset": "hotpot_qa",
      "question": "The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?",
      "prediction": "known as I Love New York, is a 2008 American romantic comedy film directed by Michael Showalter and written by Michael Showalter, David Wain, and Wain and Michael Showalter.  The film stars Zach Galif",
      "ground_truth": "Greenwich Village, New York City",
      "is_answerable": true,
      "contextual_relevance": 0.484097421169281,
      "answer_completeness": 0.2,
      "semantic_fidelity": 0.3017699122428894,
      "response_coherence": 0.7470588235294118,
      "communication_efficiency": 0.2,
      "information_throughput": 0.014671350758141494,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.632009983062744,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5abd94525542992ac4f382d2",
      "dataset": "hotpot_qa",
      "question": "2014 S/S is the debut album of a South Korean boy group that was formed by who?",
      "prediction": "debuted on August 2, 2008 with the release of their first single album \"Moonlight\".  The group consists of five members: Minho, Jonghyun, Onew, Le Sserafim,",
      "ground_truth": "YG Entertainment",
      "is_answerable": true,
      "contextual_relevance": 0.521977424621582,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.20243555307388306,
      "response_coherence": 0.776,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.657701969146729,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a85b2d95542997b5ce40028",
      "dataset": "hotpot_qa",
      "question": "Who was known by his stage name Aladin and helped organizations improve their performance as a consultant?",
      "prediction": "Paul Comer) is an American musician, songwriter, and producer. He is best known as the lead singer of the band The Commodores. He was born in Ridgeway, South Carolina, on April 13,",
      "ground_truth": "Eenasul Fateh",
      "is_answerable": true,
      "contextual_relevance": 0.14342616498470306,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.04791729524731636,
      "response_coherence": 0.9624999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.541006088256836,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a87ab905542996e4f3088c1",
      "dataset": "hotpot_qa",
      "question": "The arena where the Lewiston Maineiacs played their home games can seat how many people?",
      "prediction": "rysler, and Lehman Brothers.  The 2008 Crash was a financial crisis that began in the United States in 2007 and spread to other countries.  The crisis was caused by the subprime mort",
      "ground_truth": "3,677 seated",
      "is_answerable": true,
      "contextual_relevance": 0.016012512147426605,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.734375,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.67478084564209,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7bbb64554299042af8f7cc",
      "dataset": "hotpot_qa",
      "question": "Who is older, Annie Morton or Terry Richardson?",
      "prediction": "ord by Richardson and an essay by Gaga.  The book is a tribute to Richardson's work and the artist's relationship with him.  The book is a limited edition of 1,000 copies.",
      "ground_truth": "Terry Richardson",
      "is_answerable": true,
      "contextual_relevance": 0.2551284730434418,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.27594128251075745,
      "response_coherence": 0.732258064516129,
      "communication_efficiency": 0.5,
      "information_throughput": 0.03656338952261612,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.674880981445312,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8db19d5542994ba4e3dd00",
      "dataset": "hotpot_qa",
      "question": "Are Local H and For Against both from the United States?",
      "prediction": "13.  The Cresco Opera House was built in 1915 and opened in 1919.  It was designed by the Des Moines architectural firm of Foster & Liebbe in a combination of the",
      "ground_truth": "yes",
      "is_answerable": true,
      "contextual_relevance": 0.0,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.7419354838709677,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.715045928955078,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7166395542994082a3e814",
      "dataset": "hotpot_qa",
      "question": "What is the name of the fight song of the university whose main campus is in Lawrence, Kansas and whose branch campuses are in the Kansas City metropolitan area?",
      "prediction": "\u00edba, Brazil.  The university was founded in 1930 by the Jesuits and the Sisters of the Holy Cross.  The University of Kansas (KU) is a public research university located in Lawrence, Kansas, United",
      "ground_truth": "Kansas Song",
      "is_answerable": true,
      "contextual_relevance": 0.47809743881225586,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.3670497536659241,
      "response_coherence": 0.7272727272727273,
      "communication_efficiency": 0.5,
      "information_throughput": 0.03614635091374523,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.83265495300293,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a877e5d5542993e715abf7d",
      "dataset": "hotpot_qa",
      "question": "What screenwriter with credits for \"Evolution\" co-wrote a film starring Nicolas Cage and T\u00e9a Leoni?",
      "prediction": "Giannini.  The film is a remake of the 1974 Italian film of the same name.  The film was directed by Giuseppe Tornatore.  The film was released in the United States as \"Time to Kill",
      "ground_truth": "David Weissman",
      "is_answerable": true,
      "contextual_relevance": 0.3226715922355652,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.06808608025312424,
      "response_coherence": 0.9117647058823529,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.637643814086914,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab3b0bf5542992ade7c6e39",
      "dataset": "hotpot_qa",
      "question": "What year did Guns N Roses perform a promo for a movie starring Arnold Schwarzenegger as a former New York Police detective?",
      "prediction": "ged as the lead character.  The film was produced by Robert Evans and directed by Richard Donner.  The film was released in 1974 and was a box office failure.  The film was re-released in 1",
      "ground_truth": "1999",
      "is_answerable": true,
      "contextual_relevance": 0.412986159324646,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.19944092631340027,
      "response_coherence": 0.7057142857142857,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.610595941543579,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab56e32554299637185c594",
      "dataset": "hotpot_qa",
      "question": "Are Random House Tower and 888 7th Avenue both used for real estate?",
      "prediction": "The FBI Mortgage Fraud Department was created to investigate and prosecute mortgage fraud.  The FBI Mortgage Fraud Department was created to investigate and prosecute mortgage fraud.  The F",
      "ground_truth": "no",
      "is_answerable": true,
      "contextual_relevance": 0.1829635202884674,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.8392857142857142,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.771344900131226,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab6d09255429954757d337d",
      "dataset": "hotpot_qa",
      "question": "The football manager who recruited David Beckham managed Manchester United during what timeframe?",
      "prediction": "be one of the greatest managers in the history of the game.  Ferguson's managerial career spanned 21 seasons, during which he won 19 major trophies, including 10 Premier League titles",
      "ground_truth": "from 1986 to 2013",
      "is_answerable": true,
      "contextual_relevance": 0.46393251419067383,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1946486234664917,
      "response_coherence": 0.77,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.57345724105835,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a75e05c55429976ec32bc5f",
      "dataset": "hotpot_qa",
      "question": "Brown State Fishing Lake is in a country that has a population of how many inhabitants ?",
      "prediction": "adult.  The lake is 24 km long and 2007 resulted in a plan by the Government of Saskatchewan to lower the level of the lake by digging a drainage channel.  The Fishing",
      "ground_truth": "9,984",
      "is_answerable": true,
      "contextual_relevance": 0.5561388731002808,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.725,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.081243991851807,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab3e45655429976abd1bcd4",
      "dataset": "hotpot_qa",
      "question": "The Vermont Catamounts men's soccer team currently competes in a conference that was formerly known as what from 1988 to 1996?",
      "prediction": "the America East Conference.  They finished the season 23\u201314, 14\u201311 in America East play to finish in a tie for third place.  They advanced to the semifinals of the America East Tournament where they",
      "ground_truth": "the North Atlantic Conference",
      "is_answerable": true,
      "contextual_relevance": 0.3692694902420044,
      "answer_completeness": 0.25,
      "semantic_fidelity": 0.4355887770652771,
      "response_coherence": 0.7029411764705882,
      "communication_efficiency": 0.25,
      "information_throughput": 0.019677161806006215,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.705084323883057,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab29c24554299449642c932",
      "dataset": "hotpot_qa",
      "question": "Are Giuseppe Verdi and Ambroise Thomas both Opera composers ?",
      "prediction": "a civil airport located 10 km (6.2 mi) northwest of the city center of Parma, Italy. It is the main airport serving the region of Emilia-Romagna, and is the third busiest",
      "ground_truth": "yes",
      "is_answerable": true,
      "contextual_relevance": 0.15283873677253723,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.00989075843244791,
      "response_coherence": 0.732258064516129,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.037054300308228,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae0d4c9554299603e418468",
      "dataset": "hotpot_qa",
      "question": "Roger O. Egeberg was Assistant Secretary for Health and Scientific Affairs during the administration of a president that served during what years?",
      "prediction": "y assistant secretary for legislative affairs.  He was also a senior advisor to the U.S. Senate Committee on Armed Services.  Hood is a graduate of the University of Virginia and the University of Virginia School of Law.",
      "ground_truth": "1969 until 1974",
      "is_answerable": true,
      "contextual_relevance": 0.31861627101898193,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0923711508512497,
      "response_coherence": 0.7351351351351352,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.9876868724823,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8133725542995ce29dcbdb",
      "dataset": "hotpot_qa",
      "question": "Which writer was from England, Henry Roth or Robert Erskine Childers?",
      "prediction": "Erskine Barton Childers was a member of the Irish Republican Army.  His father was a prominent Irish nationalist and a member of the Irish Republican Brotherhood.  His mother was a member of the Irish Citizens' Army",
      "ground_truth": "Robert Erskine Childers DSC",
      "is_answerable": true,
      "contextual_relevance": 0.5978765487670898,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.5424467325210571,
      "response_coherence": 0.8666666666666666,
      "communication_efficiency": 0.5,
      "information_throughput": 0.03850233999049353,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.986223697662354,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae7a8175542993210983ed8",
      "dataset": "hotpot_qa",
      "question": "Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?",
      "prediction": "2011 Formula One season. The car is powered by a 2.0 L V8 engine, which is shared with the 2011 McLaren MP4-23. The car is designed to be",
      "ground_truth": "Pedro Rodr\u00edguez",
      "is_answerable": true,
      "contextual_relevance": 0.3450707793235779,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.17106157541275024,
      "response_coherence": 0.7357142857142858,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.974623918533325,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae6050f55429929b0807a5e",
      "dataset": "hotpot_qa",
      "question": "This singer of A Rather Blustery Day also voiced what hedgehog?",
      "prediction": "of the three previous Winnie-the-Pooh featurettes including \"Honey Tree\".  In the song Winnie the Pooh comments about the on-screen actions where he is pretending to be a \"Little Black",
      "ground_truth": "Sonic",
      "is_answerable": true,
      "contextual_relevance": 0.28127408027648926,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.10621247440576553,
      "response_coherence": 0.7689655172413793,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.997802019119263,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5adddccd5542997dc7907069",
      "dataset": "hotpot_qa",
      "question": "Aside from the Apple Remote, what other device can control the program Apple Remote was originally designed to interact with?",
      "prediction": "released in 2012 by NetSupport.  It is compatible with Windows, Mac, Linux, Solaris, and Mobile devices.  It is also compatible with the Apple TV.  It is also compatible with the Apple TV 4",
      "ground_truth": "keyboard function keys",
      "is_answerable": true,
      "contextual_relevance": 0.3737793564796448,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.061440609395504,
      "response_coherence": 0.7,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.10185694694519,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae2070a5542994d89d5b313",
      "dataset": "hotpot_qa",
      "question": "Which performance act has a higher instrument to person ratio, Badly Drawn Boy or Wolf Alice? ",
      "prediction": "got to keep going and that he was good enough.  The album was released in 2002 and was a commercial success, reaching number 19 on the UK Albums Chart.  The album's lead single \"The Boy with",
      "ground_truth": "Badly Drawn Boy",
      "is_answerable": true,
      "contextual_relevance": 0.16399112343788147,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.22535954415798187,
      "response_coherence": 0.7583333333333333,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.02543359329566196,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.10602593421936,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae22b8d554299234fd0440f",
      "dataset": "hotpot_qa",
      "question": "What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992?",
      "prediction": "the world's highest-paid athlete, earning $1.5 million per year.  He is the only player to have won the Ballon d'Or (the French Footballer of the Year award) six times.",
      "ground_truth": "World's Best Goalkeeper",
      "is_answerable": true,
      "contextual_relevance": 0.2464250773191452,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.37573540210723877,
      "response_coherence": 0.7689655172413793,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.02584590603315511,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.896949052810669,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a722b8655429971e9dc9329",
      "dataset": "hotpot_qa",
      "question": "Who was the writer of These Boots Are Made for Walkin' and who died in 2007?",
      "prediction": "in'\".  Kon Kan's debut album \"Move to Move\" was released in 1989.",
      "ground_truth": "Barton Lee Hazlewood",
      "is_answerable": true,
      "contextual_relevance": 0.15232253074645996,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0,
      "response_coherence": 0.8,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 11.796293020248413,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5adf37a95542995ec70e8f97",
      "dataset": "hotpot_qa",
      "question": "The 2011\u201312 VCU Rams men's basketball team, led by third year head coach Shaka Smart, represented Virginia Commonwealth University which was founded in what year?",
      "prediction": "Key Information: 2010; 11 VCU; 2010; 11 NCAA; 28; 12; 12; 6 in; 2011",
      "ground_truth": "1838",
      "is_answerable": true,
      "contextual_relevance": 0.6031219959259033,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.18779143691062927,
      "response_coherence": 0.7357142857142858,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.01749587059021,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5abd259d55429924427fcf1a",
      "dataset": "hotpot_qa",
      "question": "Are both Dictyosperma, and Huernia described as a genus?",
      "prediction": "Yes, both Dictyosperma and Huernia are described as a genus in the given context.",
      "ground_truth": "yes",
      "is_answerable": true,
      "contextual_relevance": 0.9628710746765137,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.09891781210899353,
      "response_coherence": 1.0,
      "communication_efficiency": 1.0,
      "information_throughput": 0.15234529737179459,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.564035892486572,
      "compression_ratio": 5.063252191170406,
      "layers_transmitted": 15,
      "bits_original": 52592640,
      "bits_compressed": 11401312,
      "bits_saved": 41191328,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a828c8355429966c78a6a50",
      "dataset": "hotpot_qa",
      "question": "Kaiser Ventures corporation was founded by an American industrialist who became known as the father of modern American shipbuilding?",
      "prediction": "was a short-lived railroad in the United States that operated from 1908 to 1912.  The EMRR was a subsidiary of the New York Shipbuilding Corporation, which was the largest",
      "ground_truth": "Henry J. Kaiser",
      "is_answerable": true,
      "contextual_relevance": 0.3522915840148926,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.09959426522254944,
      "response_coherence": 0.7379310344827587,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.951223134994507,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8a3e745542996c9b8d5e70",
      "dataset": "hotpot_qa",
      "question": "What is the name for the adventure in \"Tunnels and Trolls\", a game designed by Ken St. Andre?",
      "prediction": "name of a 1987 video game by Crank Games, which is based on \"MSPE\".  The 2013 sequel \"Wasteland 2\" is a role-playing game designed by the same",
      "ground_truth": "Arena of Khazan",
      "is_answerable": true,
      "contextual_relevance": 0.3765621781349182,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.11494491994380951,
      "response_coherence": 0.7444444444444445,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.02560520809057384,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 13.018184900283813,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ac23ff0554299636651994d",
      "dataset": "hotpot_qa",
      "question": "When was Poison's album \"Shut Up, Make Love\" released?",
      "prediction": "Keri Hilson, and Jermaine Dupri.  The song was released as the lead single from Hilson's debut album, In a Perfect World, on June 1, 2009.  The song pe",
      "ground_truth": "2000",
      "is_answerable": true,
      "contextual_relevance": 0.3279276490211487,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.13565947115421295,
      "response_coherence": 0.9678571428571427,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.864838123321533,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae4a3265542995ad6573de5",
      "dataset": "hotpot_qa",
      "question": "Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?",
      "prediction": "city of Windsor, Ontario, Canada.  It is a secondary school that serves students in grades 7 to 12.  The school was named after Dr. Frank J. Hayden, a prominent Windsor physician",
      "ground_truth": "Fujioka, Gunma",
      "is_answerable": true,
      "contextual_relevance": 0.22506389021873474,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.10953287035226822,
      "response_coherence": 0.7806451612903226,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.8807852268219,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ae0361155429925eb1afc2c",
      "dataset": "hotpot_qa",
      "question": "Which  French ace pilot and adventurer fly L'Oiseau Blanc",
      "prediction": "au Blanc\" (The White Bird).  He was killed in a crash on May 8, 1927.  The aircraft was named \"L'Oiseau Blanc\" (The White Bird) after the first flight",
      "ground_truth": "Charles Eug\u00e8ne",
      "is_answerable": true,
      "contextual_relevance": 0.5895214676856995,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.16089391708374023,
      "response_coherence": 0.7464285714285714,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.892959833145142,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5abc0a5d5542993f40c73c64",
      "dataset": "hotpot_qa",
      "question": "Are Freakonomics and In the Realm of the Hackers both American documentaries?",
      "prediction": ".  The film was released on December 19, 1995.  The film was released on DVD on December 19, 2005.  The film was released on Blu-ray on December",
      "ground_truth": "no",
      "is_answerable": true,
      "contextual_relevance": 0.13734348118305206,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1060594767332077,
      "response_coherence": 0.6333333333333333,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.962514877319336,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7cc50e554299452d57ba3e",
      "dataset": "hotpot_qa",
      "question": "Which band, Letters to Cleo or Screaming Trees, had more members?",
      "prediction": "es released after the band's breakup in 1995.  The band's music was characterized by its use of distorted guitar riffs, heavy bass lines, and driving drum beats.  The band's",
      "ground_truth": "Letters to Cleo",
      "is_answerable": true,
      "contextual_relevance": 0.2927134037017822,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.036855824291706085,
      "response_coherence": 0.7586206896551724,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.875739097595215,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5abf63f15542997ec76fd3ea",
      "dataset": "hotpot_qa",
      "question": "Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of a civil war that ended when ?",
      "prediction": "of the Communist Party of the Soviet Union (CPSU) from 1934 to 1952, and was a member of the Politburo from 1946 to 1952.  He",
      "ground_truth": "October 1922",
      "is_answerable": true,
      "contextual_relevance": 0.2973138391971588,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.23833009600639343,
      "response_coherence": 0.728,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.968045711517334,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8ef2a75542995a26add583",
      "dataset": "hotpot_qa",
      "question": "Seven Brief Lessons on Physics was written by an Italian physicist that has worked in France since what year?",
      "prediction": "a professor of physics at the University of Rome and a member of the Italian Academy of Sciences.  He was a pioneer in the field of quantum mechanics and was one of the first to use the quantum theory to explain the behavior of light.",
      "ground_truth": "2000",
      "is_answerable": true,
      "contextual_relevance": 0.5548662543296814,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.06825374811887741,
      "response_coherence": 0.6866666666666666,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.97343111038208,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a80721b554299485f5985ef",
      "dataset": "hotpot_qa",
      "question": "The Livesey Hal War Memorial commemorates the fallen of which war, that had over 60 million casualties?",
      "prediction": "by William Reid Dick and the work of Charles Wheeler. The memorial was unveiled on 25 July 1932. The memorial is located on the western side of the cemetery, opposite the entrance. The",
      "ground_truth": "World War II",
      "is_answerable": true,
      "contextual_relevance": 0.431771457195282,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.13732624053955078,
      "response_coherence": 0.7181818181818181,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.957090139389038,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab51dae5542991779162d82",
      "dataset": "hotpot_qa",
      "question": "Are both Elko Regional Airport and Gerald R. Ford International Airport located in Michigan?",
      "prediction": ".  The airport is located approximately 10 miles (16\u00a0km) north of the capital city of Bridgetown.  The airport is served by several airlines, including Air Canada, Air Canada Rouge, Air Trans",
      "ground_truth": "no",
      "is_answerable": true,
      "contextual_relevance": 0.39182615280151367,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.0384025052189827,
      "response_coherence": 0.734375,
      "communication_efficiency": 1.0,
      "information_throughput": 0.07737691565241658,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.92375111579895,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7571135542992d0ec05f98",
      "dataset": "hotpot_qa",
      "question": "Ralph Hefferline was a psychology professor at a university that is located in what city?",
      "prediction": "ology\" article published in 1968, co-authored by Maslow, stated that \"the most important thing in psychology is not to be right, but to be of use.\"  Maslow's work has been influ",
      "ground_truth": "New York City",
      "is_answerable": true,
      "contextual_relevance": 0.3011181354522705,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.07718595862388611,
      "response_coherence": 0.7709677419354839,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.869547843933105,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7a0e1e5542990783324e1a",
      "dataset": "hotpot_qa",
      "question": "Which dog's ancestors include Gordon and Irish Setters: the Manchester Terrier or the Scotch Collie?",
      "prediction": "C recognizes the Scottish Collie, the AKC does not recognize the Scottish Terrier, which is a distinct breed.  The Scottish Terrier is a small, compact dog with a long, narrow head, a short, pointed m",
      "ground_truth": "Scotch Collie",
      "is_answerable": true,
      "contextual_relevance": 0.5665025115013123,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.47837406396865845,
      "response_coherence": 0.9228571428571428,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.95044994354248,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a74106b55429979e288289e",
      "dataset": "hotpot_qa",
      "question": "Where is the company that Sachin Warrier worked for as a software engineer headquartered? ",
      "prediction": "Shaan Rahman.  She has sung in many Malayalam movies like \"Shaan Rahman debut album\" \"Coffee @ MG Road\" \"Vineeth Sreenivasan - Shaan",
      "ground_truth": "Mumbai",
      "is_answerable": true,
      "contextual_relevance": 0.2282440960407257,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.27529799938201904,
      "response_coherence": 0.9863636363636363,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.896038055419922,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a79311755429970f5fffe67",
      "dataset": "hotpot_qa",
      "question": "A Japanese manga series based on a 16 year old high school student Ichitaka Seto, is written and illustrated by someone born in what year?",
      "prediction": "be illustrated by Kazuki Takahashi.  The series is set in the fictional town of Bethlehem, where the main character, a young man named Yuki, is a member of the Bethlehem Baptist Church.",
      "ground_truth": "1962",
      "is_answerable": true,
      "contextual_relevance": 0.5115055441856384,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.018266668543219566,
      "response_coherence": 0.74375,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.957626104354858,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab2d3df554299194fa9352c",
      "dataset": "hotpot_qa",
      "question": "The battle in which Giuseppe Arimondi lost his life secured what for Ethiopia?",
      "prediction": "inces\"), Ethiopia became a unified monarchy in 1830 with the ascension of Emperor Menelik II.  The 19th century saw Ethiopia's rise as a major power in the Horn",
      "ground_truth": "sovereignty",
      "is_answerable": true,
      "contextual_relevance": 0.3814542293548584,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.12579520046710968,
      "response_coherence": 0.7571428571428571,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.819049835205078,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a760ab65542994ccc918697",
      "dataset": "hotpot_qa",
      "question": "Alfred Balk served as the secretary of the Committee on the Employment of Minority Groups in the News Media under which United States Vice President?",
      "prediction": "President\u2019s place.  The pro tempore is chosen by the Senate, and the Senate chooses the pro tempore from among its members.  The pro tempore is not a member of the Senate, and is not entitled to vote in",
      "ground_truth": "Nelson Rockefeller",
      "is_answerable": true,
      "contextual_relevance": 0.1829032003879547,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.08389347791671753,
      "response_coherence": 0.8894736842105262,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.995643138885498,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a7d54165542995f4f402256",
      "dataset": "hotpot_qa",
      "question": "A medieval fortress in Dirleton, East Lothian, Scotland borders on the south side of what coastal area?",
      "prediction": "town.  The town was granted a charter of incorporation in 1215, and in 1232 it was granted a market and fair.  The town was granted a charter of freedom in 129",
      "ground_truth": "Yellowcraig",
      "is_answerable": true,
      "contextual_relevance": 0.04189998656511307,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.03384237736463547,
      "response_coherence": 0.6741935483870968,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.919074058532715,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5ab859a955429934fafe6d7b",
      "dataset": "hotpot_qa",
      "question": "Who is the writer of this song that was inspired by words on a tombstone and was the first track on the box set Back to Mono?",
      "prediction": "five tracks from his sixteen studio albums, plus a live album and a collection of unreleased material.  The box set was released on 10 December 2006.  The box set includes a 40-page",
      "ground_truth": "Phil Spector",
      "is_answerable": true,
      "contextual_relevance": 0.36004626750946045,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.04235992580652237,
      "response_coherence": 0.753125,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.934676885604858,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5add61d65542995b365fab21",
      "dataset": "hotpot_qa",
      "question": "What type of forum did a former Soviet statesman initiate?",
      "prediction": "Zinoviev, and Sklyansky was appointed to the Politburo of the Communist Party of the Soviet Union.  He was a member of the Central Committee of the Communist Party of the Soviet Union from 192",
      "ground_truth": "Organizations could come together to address global issues",
      "is_answerable": true,
      "contextual_relevance": 0.4757993817329407,
      "answer_completeness": 0.125,
      "semantic_fidelity": 0.02821802906692028,
      "response_coherence": 0.8852941176470588,
      "communication_efficiency": 0.125,
      "information_throughput": 0.00968110811580163,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.911745071411133,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5a8e068b5542995085b37384",
      "dataset": "hotpot_qa",
      "question": "Are Ferocactus and Silene both types of plant?",
      "prediction": "native of the eastern United States and Canada.  It is a perennial herb growing from a caudex, appearing matlike, decumbent, or erect, with stems a few centimeters to over half a meter",
      "ground_truth": "yes",
      "is_answerable": true,
      "contextual_relevance": 0.38987284898757935,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.02846462093293667,
      "response_coherence": 0.771875,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.89497685432434,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "5abbf698554299114383a0b5",
      "dataset": "hotpot_qa",
      "question": "Which British first-generation jet-powered medium bomber was used in the South West Pacific theatre of World War II?",
      "prediction": "Asia was the first theatre of operations for the Tupolev Tu-12, and it saw service in the Korean War.\n\nQuestion: What was the name of the Italian medium bomber designed by Savoia-Marchetti",
      "ground_truth": "English Electric Canberra",
      "is_answerable": true,
      "contextual_relevance": 0.5088339447975159,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.08318675309419632,
      "response_coherence": 0.94375,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 12.820499897003174,
      "compression_ratio": 5.065238895049111,
      "layers_transmitted": 15,
      "bits_original": 125829120,
      "bits_compressed": 27269216,
      "bits_saved": 98559904,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    }
  ],
  "narrativeqa": [
    {
      "id": "narrativeqa_0",
      "dataset": "narrativeqa",
      "question": "WHO NORMALLY DELIVERS THE OPENING PROLOGUE IN THE PLAY?",
      "prediction": "The play begins with three pages disputing over the black cloak usually worn by the actor who delivers the prologue. They draw lots for the cloak, and one of the losers, Anaides, starts telling the audience what happens",
      "ground_truth": "THE ACTOR WEARING THE BLACK CLOAK",
      "is_answerable": true,
      "contextual_relevance": 0.6182628870010376,
      "answer_completeness": 0.8,
      "semantic_fidelity": 0.4756176471710205,
      "response_coherence": 0.9526315789473683,
      "communication_efficiency": 0.8,
      "information_throughput": 0.09301720715785833,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.60055923461914,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_1",
      "dataset": "narrativeqa",
      "question": "WHAT NAME WAS CYNTHIA MORE FAMOUSLY KNOWN BY?",
      "prediction": "Cynthia is more famous by her nickname, Narcissus.",
      "ground_truth": "THE GODDESS DIANA",
      "is_answerable": true,
      "contextual_relevance": 0.7825337052345276,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.35015881061553955,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.1966872215271,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_2",
      "dataset": "narrativeqa",
      "question": "WHO DOES ECHO WEEP FOR?",
      "prediction": "The question asks who Echo weeps for in the play proper. In the play proper, Echo weeps for Narcissus, the protagonist of the play.",
      "ground_truth": "NARCISSUS",
      "is_answerable": true,
      "contextual_relevance": 0.7614942193031311,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.3762369751930237,
      "response_coherence": 0.8875,
      "communication_efficiency": 1.0,
      "information_throughput": 0.1227175179316534,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.14879584312439,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_3",
      "dataset": "narrativeqa",
      "question": "WHAT DOES A DRINK FROM NARCISSUS'S SPRING CAUSE THE DRINKER TO DO?",
      "prediction": "The drink from Narcissus's spring causes the drinker to grow dotingly enamored of themselves.",
      "ground_truth": "FALL IN LOVE WITH THEMSELVES",
      "is_answerable": true,
      "contextual_relevance": 0.8709061145782471,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.34964659810066223,
      "response_coherence": 0.9785714285714285,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.590385913848877,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_4",
      "dataset": "narrativeqa",
      "question": "IN WHAT VALLEY DID THE SOLEMN REVELS OF CYNTHIA TAKE PLACE?",
      "prediction": "The play proper takes place in the valley of Gargaphie in Greece.",
      "ground_truth": "GARGAPHIE IN GREECE",
      "is_answerable": true,
      "contextual_relevance": 0.2860572338104248,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.6803991794586182,
      "response_coherence": 0.95,
      "communication_efficiency": 1.0,
      "information_throughput": 0.13879096352994028,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.205080032348633,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_5",
      "dataset": "narrativeqa",
      "question": "WHAT DID THE  SYMBOLIC VICES DISGUISE THEMSELVES TO BE?",
      "prediction": "The symbolic vices disguise themselves to be the three pages disputing over the black cloak usually worn by the actor who delivers the prologue and the others trying to suppress Anaides, who starts telling the audience what happens in",
      "ground_truth": "VIRTUES",
      "is_answerable": true,
      "contextual_relevance": 0.6781758069992065,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.12494948506355286,
      "response_coherence": 0.7384615384615385,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.482487916946411,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_6",
      "dataset": "narrativeqa",
      "question": "WHAT SENTENCE DID CYNTHIA GIVE TO THE SYMBOLIC VICES?",
      "prediction": "Cynthia gives the symbolic vices to the losers of the lottery for the cloak worn by the actor who delivers the prologue.",
      "ground_truth": "TO BATHE IN THE SPRING OF HELICON",
      "is_answerable": true,
      "contextual_relevance": 0.6133796572685242,
      "answer_completeness": 0.42857142857142855,
      "semantic_fidelity": 0.11325568705797195,
      "response_coherence": 0.9318181818181818,
      "communication_efficiency": 0.42857142857142855,
      "information_throughput": 0.05119995104171348,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.370543718338013,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_7",
      "dataset": "narrativeqa",
      "question": "WHO DRANK FROM THE SPRING AT CYNTHIA'S REVELS?",
      "prediction": "The question asks who drank from the spring at Cynthia's revels. The context suggests that the play proper begins with three pages disputing over the black cloak usually worn by the actor who delivers the prologue,",
      "ground_truth": "ALL THE COURTIERS AND LADIES WHO ATTENDED CYNTHIA'S REVELS",
      "is_answerable": true,
      "contextual_relevance": 0.7070250511169434,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.46505993604660034,
      "response_coherence": 0.9416666666666667,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.03678056995789623,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.062756061553955,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_8",
      "dataset": "narrativeqa",
      "question": "HOW MANY PHASES DID THE COURT COMPLIMENT COMPETITION HAVE?",
      "prediction": "The play proper in the context of the given text has three phases: disputing over the black cloak usually worn by the actor who delivers the prologue, one of the losers, Anaides, starting to tell the audience what happens",
      "ground_truth": "4 PHASES",
      "is_answerable": true,
      "contextual_relevance": 0.29634684324264526,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.281663179397583,
      "response_coherence": 0.7384615384615385,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.025048732757568,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_9",
      "dataset": "narrativeqa",
      "question": "WHO CHALLENGES THE COURTIERS TO COURT COMPLIMENT COMPETITION?",
      "prediction": "In the play proper, the goddess Diana, also called Cynthia, has ordained a \"solemn revels\" in the valley of Gargaphie in Greece. The gods Cupid and Mercury appear, and they",
      "ground_truth": "ASOTUS",
      "is_answerable": true,
      "contextual_relevance": 0.14430883526802063,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.10259130597114563,
      "response_coherence": 0.94,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 9.11884069442749,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_10",
      "dataset": "narrativeqa",
      "question": "How many phases does the competition have?",
      "prediction": "The competition in the play begins with three pages disputing over the black cloak usually worn by the actor who delivers the prologue. They draw lots for the cloak, and one of the losers, Anaides, starts telling the",
      "ground_truth": "Four.",
      "is_answerable": true,
      "contextual_relevance": 0.38250041007995605,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.08884752541780472,
      "response_coherence": 0.9447368421052631,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.884639978408813,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_11",
      "dataset": "narrativeqa",
      "question": "Who does Diana represent?",
      "prediction": "Diana is not explicitly represented in the given context. The play is about a mythological figure named Cynthia, who is described as the goddess of the valley of Gargaphie in Greece. The goddess Cynthia",
      "ground_truth": "Queen Elizabeth.",
      "is_answerable": true,
      "contextual_relevance": 0.7130508422851562,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.2515004277229309,
      "response_coherence": 0.9205882352941176,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.827504873275757,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_12",
      "dataset": "narrativeqa",
      "question": "Who is sleeping at the beginning of the play?",
      "prediction": "The question asks who is sleeping at the beginning of the play. The answer is that the three pages disputing over the black cloak usually worn by the actor who delivers the prologue are sleeping at the beginning of the play.",
      "ground_truth": "Echo",
      "is_answerable": true,
      "contextual_relevance": 0.7358651161193848,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.09483075141906738,
      "response_coherence": 0.8829268292682926,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.964008331298828,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_13",
      "dataset": "narrativeqa",
      "question": "Where is Narcissus's spring?",
      "prediction": "In the play proper, the goddess Diana, also called Cynthia, has ordained a \"solemn revels\" in the valley of Gargaphie in Greece. The gods Cupid and Mercury appear, and they",
      "ground_truth": "Gargaphie Valley Greece.",
      "is_answerable": true,
      "contextual_relevance": 0.170634463429451,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.43436723947525024,
      "response_coherence": 0.94,
      "communication_efficiency": 1.0,
      "information_throughput": 0.11222444160547235,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.910714864730835,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_14",
      "dataset": "narrativeqa",
      "question": "Which two societies are aligned in this play?",
      "prediction": "The two societies aligned in this play are the gods Cupid and Mercury and the courtiers and ladies who attend the Cynthia's revels.",
      "ground_truth": "Greek mythology and Queen Elizabeth's Court",
      "is_answerable": true,
      "contextual_relevance": 0.5530521273612976,
      "answer_completeness": 0.16666666666666666,
      "semantic_fidelity": 0.48813596367836,
      "response_coherence": 0.9347826086956521,
      "communication_efficiency": 0.16666666666666666,
      "information_throughput": 0.0204088494493022,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.166392087936401,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_15",
      "dataset": "narrativeqa",
      "question": "Where do the dancers purify themselves?",
      "prediction": "The dancers purify themselves in the spring of Gargaphie in Greece, as described in the context.",
      "ground_truth": "Mount Helicon",
      "is_answerable": true,
      "contextual_relevance": 0.7248039245605469,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0344802625477314,
      "response_coherence": 0.9249999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.542909860610962,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_16",
      "dataset": "narrativeqa",
      "question": "Which page performs the dialogue?",
      "prediction": "The page disputing over the black cloak usually worn by the actor who delivers the prologue is the page that performs the dialogue.",
      "ground_truth": "Anaides",
      "is_answerable": true,
      "contextual_relevance": 0.6133996248245239,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1436767727136612,
      "response_coherence": 0.9217391304347825,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.894151926040649,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_17",
      "dataset": "narrativeqa",
      "question": "Who enters with Mercury?",
      "prediction": "In the play proper, the goddess Diana, also called Cynthia, has ordained a \"solemn revels\" in the valley of Gargaphie in Greece. The gods Cupid and Mercury appear, and they",
      "ground_truth": "Cupid",
      "is_answerable": true,
      "contextual_relevance": 0.3162102699279785,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.3606390058994293,
      "response_coherence": 0.94,
      "communication_efficiency": 1.0,
      "information_throughput": 0.11177628071902702,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.946441888809204,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_18",
      "dataset": "narrativeqa",
      "question": "Who is the 2nd Earl of Essex?",
      "prediction": "The 2nd Earl of Essex is not mentioned in the given context.",
      "ground_truth": "Robert Devereux",
      "is_answerable": true,
      "contextual_relevance": 0.8236534595489502,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.0931633859872818,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.046967029571533,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_19",
      "dataset": "narrativeqa",
      "question": "From where does this play originate? ",
      "prediction": "The play originates from the ancient Greek playwright Aeschylus's tragedy \"The Persians,\" which was first performed in the 5th century BCE. The play's dialogue and themes are similar to",
      "ground_truth": "Church choir",
      "is_answerable": true,
      "contextual_relevance": 0.5127078294754028,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.023273643106222153,
      "response_coherence": 0.9689655172413792,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.795323848724365,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_20",
      "dataset": "narrativeqa",
      "question": "What are people arguing about as the story begins?",
      "prediction": "People are arguing over the black cloak usually worn by the actor who delivers the prologue in the play. They are disputing over who gets to wear the cloak and who gets to keep it. The play proper begins with",
      "ground_truth": "A black cloak ",
      "is_answerable": true,
      "contextual_relevance": 0.526861846446991,
      "answer_completeness": 0.6666666666666666,
      "semantic_fidelity": 0.533469557762146,
      "response_coherence": 0.9099999999999999,
      "communication_efficiency": 0.6666666666666666,
      "information_throughput": 0.07577966547129927,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.79743480682373,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_21",
      "dataset": "narrativeqa",
      "question": "Who announces the events that are to come to the dismay of the others on stage?",
      "prediction": "The play proper begins with three pages disputing over the black cloak usually worn by the actor who delivers the prologue. They draw lots for the cloak, and one of the losers, Anaides, starts telling the audience what",
      "ground_truth": "Anaides",
      "is_answerable": true,
      "contextual_relevance": 0.31392350792884827,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.3325458765029907,
      "response_coherence": 0.9526315789473683,
      "communication_efficiency": 1.0,
      "information_throughput": 0.11327052774866028,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.828421831130981,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_22",
      "dataset": "narrativeqa",
      "question": "What is another name for the Goddess Diana?",
      "prediction": "Another name for the Goddess Diana is Cynthia.",
      "ground_truth": "Cynthia",
      "is_answerable": true,
      "contextual_relevance": 0.8554731011390686,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.6165706515312195,
      "response_coherence": 1.0,
      "communication_efficiency": 1.0,
      "information_throughput": 0.14396227898766775,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.946264028549194,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_23",
      "dataset": "narrativeqa",
      "question": "What does Diana order to take place  in the valley of Gargaphie?",
      "prediction": "Diana orders a \"solemn revels\" in the valley of Gargaphie in Greece.",
      "ground_truth": "Solemn revels",
      "is_answerable": true,
      "contextual_relevance": 0.783362865447998,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.5556488037109375,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 1.0,
      "information_throughput": 0.13751201421680684,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.272091865539551,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_24",
      "dataset": "narrativeqa",
      "question": "Why does Asotus become emboldened by vanity and self -love?",
      "prediction": "Key Information: Asotus, a foolish spendthrift who longs to become a courtier and a master of fashion and manners, becomes emboldened by vanity and self-love.\n\nContext: The play proper",
      "ground_truth": "He drank from Narcissus' spring ",
      "is_answerable": true,
      "contextual_relevance": 0.7136656045913696,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.2815217673778534,
      "response_coherence": 0.9586206896551723,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.601567268371582,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_25",
      "dataset": "narrativeqa",
      "question": "What challenge does Asotus propose to all comers?",
      "prediction": "Asotus, a foolish spendthrift who longs to become a courtier and a master of fashion and manners, proposes to all comers in the play proper that they drink from the spring. This challenge is seen as a",
      "ground_truth": "\"Court Compliment\"",
      "is_answerable": true,
      "contextual_relevance": 0.66800856590271,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.2934330105781555,
      "response_coherence": 0.9513513513513513,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.621575832366943,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_26",
      "dataset": "narrativeqa",
      "question": "What does Diana tell the courtiers to do after the dance?",
      "prediction": "Diana tells the courtiers to \"embolden\" themselves by drinking from the spring.",
      "ground_truth": "Purify themselves by bathing in a sacred spring",
      "is_answerable": true,
      "contextual_relevance": 0.6334929466247559,
      "answer_completeness": 0.25,
      "semantic_fidelity": 0.2926786541938782,
      "response_coherence": 0.9749999999999999,
      "communication_efficiency": 0.25,
      "information_throughput": 0.03433225027873082,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.281783103942871,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_27",
      "dataset": "narrativeqa",
      "question": "What effect does the spring of Narcissus have on those who drink from it?",
      "prediction": "The spring of Narcissus has a profound effect on those who drink from it. It causes them to grow dotingly enamored of themselves, leading to their becoming envious and jealous of others. This is evident in the cour",
      "ground_truth": "Grow enamored of themselves",
      "is_answerable": true,
      "contextual_relevance": 0.8682271242141724,
      "answer_completeness": 0.75,
      "semantic_fidelity": 0.3644368052482605,
      "response_coherence": 0.9684210526315788,
      "communication_efficiency": 0.75,
      "information_throughput": 0.08712879777371936,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.607946157455444,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_28",
      "dataset": "narrativeqa",
      "question": "Who awakens Echo?",
      "prediction": "The question asks who awakens Echo in the context of the play proper. The answer is Mercury, who awakens Echo after she weeps for Narcissus.",
      "ground_truth": "Mercury",
      "is_answerable": true,
      "contextual_relevance": 0.7005999088287354,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.33303314447402954,
      "response_coherence": 0.9279999999999999,
      "communication_efficiency": 1.0,
      "information_throughput": 0.12317086969275617,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.118802785873413,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_29",
      "dataset": "narrativeqa",
      "question": "Who does Cynthia represent in the masque?",
      "prediction": "Cynthia represents the goddess Diana, also called Cynthia, in the masque. She is the goddess of the hunt, the moon, and the seasons, and her name means \"the goddess of the",
      "ground_truth": "Queen Elizabeth",
      "is_answerable": true,
      "contextual_relevance": 0.7167614698410034,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.21034862101078033,
      "response_coherence": 0.9032258064516129,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.595041036605835,
      "compression_ratio": 5.06386667764384,
      "layers_transmitted": 15,
      "bits_original": 64143360,
      "bits_compressed": 13903968,
      "bits_saved": 50239392,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_30",
      "dataset": "narrativeqa",
      "question": "Why did Micky feel he lost the fight in Atlantic City?",
      "prediction": "Micky felt he lost the fight in Atlantic City because his scheduled opponent, who was 20 pounds heavier than him, was found to be a substitute who was 20 pounds heavier than Micky. This difference in professional box",
      "ground_truth": "Opponent was in a weight class of 20 pounds heavier.",
      "is_answerable": true,
      "contextual_relevance": 0.7417647242546082,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.6138482093811035,
      "response_coherence": 0.9368421052631578,
      "communication_efficiency": 0.5,
      "information_throughput": 0.05922891295681048,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.44182300567627,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_31",
      "dataset": "narrativeqa",
      "question": "What injury does Micky receive as he attempted to help his step-brother in a fight with police?",
      "prediction": "Micky receives an injury as he attempts to help his step-brother in a fight with police.",
      "ground_truth": "Broken hand",
      "is_answerable": true,
      "contextual_relevance": 0.9075164794921875,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.25211843848228455,
      "response_coherence": 1.0,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.83735990524292,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_32",
      "dataset": "narrativeqa",
      "question": "What does Dicky's family realize to be the destroyer in Dicky's life and career after seeing a documentary on HBO?",
      "prediction": "Dicky's family realizes that Dicky's lack of motivation is due to the fact that he has been struggling to find a job and has been struggling to pay for his mother's medical bills. They blame Charl",
      "ground_truth": "Crack",
      "is_answerable": true,
      "contextual_relevance": 0.6995354890823364,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.1619395911693573,
      "response_coherence": 0.9416666666666667,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.241000890731812,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_33",
      "dataset": "narrativeqa",
      "question": "Who is the manager that gets Micky away from his family of bad influence and encourages him back into the ring?",
      "prediction": "The manager that gets Micky away from his family of bad influence and encourages him back into the ring is Dicky.",
      "ground_truth": "Sal Lanano",
      "is_answerable": true,
      "contextual_relevance": 0.875200629234314,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.108095183968544,
      "response_coherence": 0.9857142857142857,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.169015169143677,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_34",
      "dataset": "narrativeqa",
      "question": "During his series of fights to regain his career, whose advice does Micky follow to gain momentum against his opponent?",
      "prediction": "During his series of fights to regain his career, Micky follows the advice of his mother and brother, who both agree that he should focus on his training and not worry about the opponent's weight class.",
      "ground_truth": "his brother, Dicky",
      "is_answerable": true,
      "contextual_relevance": 0.7419513463973999,
      "answer_completeness": 0.6666666666666666,
      "semantic_fidelity": 0.33728620409965515,
      "response_coherence": 0.95,
      "communication_efficiency": 0.6666666666666666,
      "information_throughput": 0.08307567183669275,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.0248122215271,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_35",
      "dataset": "narrativeqa",
      "question": "Which member of Micky's family realize that other members of the family are bad influences?",
      "prediction": "The context suggests that Micky's mother and seven sisters blame Charlene for his lack of motivation, but Dicky tries to get money by posing as a prostitute and impersonating a police officer to steal money",
      "ground_truth": "Micky's father",
      "is_answerable": true,
      "contextual_relevance": 0.5487060546875,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.4990306496620178,
      "response_coherence": 0.7657142857142857,
      "communication_efficiency": 0.5,
      "information_throughput": 0.061623194805597026,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 8.11382794380188,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_36",
      "dataset": "narrativeqa",
      "question": "Who does Dicky seek out to work with in reuniting the family?",
      "prediction": "Dicky seeks out Charlene Fleming to work with in reuniting the family.",
      "ground_truth": "Charlene",
      "is_answerable": true,
      "contextual_relevance": 0.7170628309249878,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.45852187275886536,
      "response_coherence": 1.0,
      "communication_efficiency": 1.0,
      "information_throughput": 0.14774697786157562,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.768327951431274,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_37",
      "dataset": "narrativeqa",
      "question": "In what location does the group go for the final title fight?",
      "prediction": "The group goes for the final title fight in Las Vegas.",
      "ground_truth": "London",
      "is_answerable": true,
      "contextual_relevance": 0.756267786026001,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.09507729113101959,
      "response_coherence": 0.9727272727272727,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.280692100524902,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_38",
      "dataset": "narrativeqa",
      "question": "What reward does Micky gain in the final victory?",
      "prediction": "In the final victory, Micky gains a purse of two or three weight classes, which is a significant difference in professional boxing.",
      "ground_truth": "The welterweight title",
      "is_answerable": true,
      "contextual_relevance": 0.6233964562416077,
      "answer_completeness": 0.3333333333333333,
      "semantic_fidelity": 0.4133072793483734,
      "response_coherence": 0.9727272727272727,
      "communication_efficiency": 0.3333333333333333,
      "information_throughput": 0.04736548688287365,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.03747296333313,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_39",
      "dataset": "narrativeqa",
      "question": "What is Charlene Fleming's current occupation?",
      "prediction": "Charlene Fleming's current occupation is a former college athlete who dropped out and became a bartender.",
      "ground_truth": "A bartender",
      "is_answerable": true,
      "contextual_relevance": 0.7009502053260803,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.4577847719192505,
      "response_coherence": 0.98125,
      "communication_efficiency": 1.0,
      "information_throughput": 0.14450296340522703,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.920273303985596,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_40",
      "dataset": "narrativeqa",
      "question": "How much heavier than Micky is Micky's opponent in Atlantic City?",
      "prediction": "Micky's opponent in Atlantic City is 20 pounds heavier than Micky, which constitutes two or three weight classes.",
      "ground_truth": "Twenty pounds",
      "is_answerable": true,
      "contextual_relevance": 0.8707371950149536,
      "answer_completeness": 0.5,
      "semantic_fidelity": 0.45024657249450684,
      "response_coherence": 1.0,
      "communication_efficiency": 0.5,
      "information_throughput": 0.07126343079835253,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.016221284866333,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_41",
      "dataset": "narrativeqa",
      "question": "Why do Micky and his family agree to the fight in Atlantic City?",
      "prediction": "Micky and his family agree to the fight in Atlantic City because they want to get the purse and Micky is defeated, which constitutes two or three weight classes. Despite Micky's reservations, his mother and brother agree so",
      "ground_truth": "To win the purse",
      "is_answerable": true,
      "contextual_relevance": 0.7858313918113708,
      "answer_completeness": 0.75,
      "semantic_fidelity": 0.2973744869232178,
      "response_coherence": 0.9447368421052631,
      "communication_efficiency": 0.75,
      "information_throughput": 0.09440329423360569,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.944638013839722,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_42",
      "dataset": "narrativeqa",
      "question": "What does Dicky have his girlfriend pose as?",
      "prediction": "Dicky has his girlfriend pose as a prostitute and then impersonates a police officer to steal the client's money.",
      "ground_truth": "A Prostitute",
      "is_answerable": true,
      "contextual_relevance": 0.6571580767631531,
      "answer_completeness": 1.0,
      "semantic_fidelity": 0.44918134808540344,
      "response_coherence": 0.9842105263157894,
      "communication_efficiency": 1.0,
      "information_throughput": 0.14362362298333267,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.9626429080963135,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_43",
      "dataset": "narrativeqa",
      "question": "Who does Micky accuse of favoring Dicky?",
      "prediction": "Micky accuses Dicky of favoring him by offering him a fight in Las Vegas, but Dicky denies it.",
      "ground_truth": "His mother",
      "is_answerable": true,
      "contextual_relevance": 0.8118953704833984,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.21112361550331116,
      "response_coherence": 0.9666666666666666,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.993601083755493,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_44",
      "dataset": "narrativeqa",
      "question": "Where does Micky's last fight occur?",
      "prediction": "Micky's last fight occurs in Atlantic City, where he is scheduled to face an opponent who is ill and a substitute who is 20 pounds heavier than Micky, a huge difference in professional boxing.",
      "ground_truth": "London",
      "is_answerable": true,
      "contextual_relevance": 0.7793684601783752,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.16302275657653809,
      "response_coherence": 0.9558823529411764,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.788625240325928,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_45",
      "dataset": "narrativeqa",
      "question": "What happens to Dicky when he poses as a policeman?",
      "prediction": "Dicky poses as a policeman to steal money from a client, but he is caught by the client's girlfriend and is arrested. This leads to Dicky's arrest and imprisonment, and he is forced to leave his",
      "ground_truth": "He is arrested",
      "is_answerable": true,
      "contextual_relevance": 0.7824822664260864,
      "answer_completeness": 0.6666666666666666,
      "semantic_fidelity": 0.4962944984436035,
      "response_coherence": 0.9333333333333333,
      "communication_efficiency": 0.6666666666666666,
      "information_throughput": 0.08352839304473597,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.98131799697876,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_46",
      "dataset": "narrativeqa",
      "question": "What title does MIcky finally win?",
      "prediction": "Micky finally wins the undercard fight in Atlantic City, despite being scheduled to fight 20 pounds heavier than his opponent and being 20 pounds heavier than the substitute.",
      "ground_truth": "Welterweight Champion",
      "is_answerable": true,
      "contextual_relevance": 0.650688886642456,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.3335452675819397,
      "response_coherence": 0.9249999999999999,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.508668899536133,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_47",
      "dataset": "narrativeqa",
      "question": "Who suffers from a crack addiction?",
      "prediction": "The character who suffers from a crack addiction is Micky, who is described as having reservations about his scheduled opponent being ill and a substitute being found who is 20 pounds heavier than him, a huge difference in professional",
      "ground_truth": "Dicky",
      "is_answerable": true,
      "contextual_relevance": 0.5108590126037598,
      "answer_completeness": 0.0,
      "semantic_fidelity": 0.21176156401634216,
      "response_coherence": 0.7461538461538462,
      "communication_efficiency": 0.0,
      "information_throughput": 0.0,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 7.972905874252319,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_48",
      "dataset": "narrativeqa",
      "question": "What criminal crime did dickey commit that caused him to get arrested ?",
      "prediction": "Dicky committed the crime of impersonating a police officer to steal money from a client.",
      "ground_truth": "Dickey had his girlfriend pose as a prostitute, then pretend to be a cop to extort money",
      "is_answerable": true,
      "contextual_relevance": 0.7496737241744995,
      "answer_completeness": 0.2,
      "semantic_fidelity": 0.7347583770751953,
      "response_coherence": 0.98,
      "communication_efficiency": 0.2,
      "information_throughput": 0.030105750808055443,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.643249034881592,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    },
    {
      "id": "narrativeqa_49",
      "dataset": "narrativeqa",
      "question": "How did Mickey break his hand?",
      "prediction": "In the given context, Mickey broke his hand during an undercard fight in Atlantic City.",
      "ground_truth": "It was broken by the police officer that arrested him while he was trying to defend his brother from police ",
      "is_answerable": true,
      "contextual_relevance": 0.7927591800689697,
      "answer_completeness": 0.1111111111111111,
      "semantic_fidelity": 0.35798710584640503,
      "response_coherence": 0.98,
      "communication_efficiency": 0.1111111111111111,
      "information_throughput": 0.01706723710239892,
      "compression_quality_score": null,
      "semantic_preservation": null,
      "inference_time": 6.510199069976807,
      "compression_ratio": 5.063659408996568,
      "layers_transmitted": 15,
      "bits_original": 59719680,
      "bits_compressed": 12945504,
      "bits_saved": 46774176,
      "baseline_prediction": null,
      "baseline_time": null,
      "using_semantic_model": true,
      "success": true
    }
  ]
}