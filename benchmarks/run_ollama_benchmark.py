"""Run benchmark with local models - 5 datasets with proper agentic metrics"""

import os
import sys

import torch

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from benchmark_suite import BenchmarkSuite
from transformers import AutoModelForCausalLM, AutoTokenizer

from q_kvcomm import QKVCommConfig, QKVCommSystem


def main():
    print("=" * 80)
    print("Q-KVCOMM AGENTIC COMMUNICATION BENCHMARK")
    print("=" * 80)
    print("\nüìö Datasets:")
    print("  1. SQuAD 2.0 - Extractive QA (answerable + unanswerable)")
    print("  2. HotpotQA - Multi-hop reasoning")
    print("  3. Natural Questions - Open domain QA")
    print("  4. CoQA - Conversational QA")
    print("  5. NarrativeQA - Reading comprehension")
    print("\nüìä Evaluation Framework:")
    print("  ‚Ä¢ Contextual Relevance (question-answer relevance)")
    print("  ‚Ä¢ Answer Completeness (coverage of ground truth)")
    print("  ‚Ä¢ Semantic Fidelity (meaning preservation)")
    print("  ‚Ä¢ Response Coherence (output quality)")
    print("  ‚Ä¢ Communication Efficiency (quality per bit)")
    print("  ‚Ä¢ Information Throughput (quality per second)")
    print("\nüî¨ Compression Analysis:")
    print("  ‚Ä¢ Compression Quality Score (vs baseline)")
    print("  ‚Ä¢ Semantic Preservation (compressed vs uncompressed)")
    print("  ‚Ä¢ Bandwidth savings and layer efficiency")
    print("=" * 80)

    # Device configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nüñ•Ô∏è  Device: {device}")

    # Model selection - use instruction-tuned for better agentic communication
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    print(f"ü§ñ Model: {model_name}")

    # Load tokenizer
    print("\n‚è≥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("‚úì Tokenizer loaded")

    # Load sender model
    print("\n‚è≥ Loading sender model...")
    sender = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32 if device == "cpu" else torch.float16,
        device_map=device,
    )
    sender.tokenizer = tokenizer
    print("‚úì Sender model loaded")

    # Load receiver model (same for homogeneous setup)
    print("\n‚è≥ Loading receiver model...")
    receiver = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32 if device == "cpu" else torch.float16,
        device_map=device,
    )
    receiver.tokenizer = tokenizer
    print("‚úì Receiver model loaded")

    # Configure Q-KVComm for optimal agentic communication
    print("\n‚öôÔ∏è  Configuring Q-KVComm system...")
    config = QKVCommConfig(
        # Mode
        mode="full",  # Full Q-KVComm with quantization + calibration
        
        # Quantization settings - balanced for quality/compression
        target_bits=6.0,  # Target 6-bit (can adjust: 4-8 range)
        min_bits=4,
        max_bits=8,
        quantization_enabled=True,
        profiling_samples=50,  # Samples for sensitivity profiling
        
        # Calibration settings
        calibration_enabled=True,
        calibration_samples=30,  # Samples for feature calibration
        
        # Layer selection - preserve more layers for quality
        layer_selection_ratio=0.7,  # Use 70% of layers
        attention_weight=0.5,  # Balance attention + Gaussian prior
        
        # Information extraction
        extraction_method="yake",  # YAKE for keyword extraction
        extraction_max_tokens=50,
        extraction_min_confidence=0.5,
        extraction_cache_enabled=True,
        
        # Memory management
        max_memory_mb=1024.0,  # 1GB cache
        enable_disk_cache=True,
        adaptive_compression=True,
    )
    
    print("‚úì Configuration loaded")
    print(f"  ‚Ä¢ Target bits: {config.target_bits}")
    print(f"  ‚Ä¢ Layer selection: {config.layer_selection_ratio * 100:.0f}%")
    print(f"  ‚Ä¢ Extraction method: {config.extraction_method}")

    # Initialize Q-KVComm system
    print("\nüîß Initializing Q-KVComm system...")
    qkvcomm = QKVCommSystem(sender, receiver, config, device)
    print("‚úì Q-KVComm system initialized")

    # Create benchmark suite
    print("\nüìã Setting up benchmark suite...")
    
    # Set enable_baseline=True to compare compressed vs uncompressed
    # This is CRITICAL for research evaluation but slower (2x time)
    enable_baseline = True
    
    benchmark = BenchmarkSuite(
        qkvcomm_system=qkvcomm,
        output_dir="benchmark_results",
        enable_baseline=enable_baseline,
    )
    print("‚úì Benchmark suite ready")
    
    if enable_baseline:
        print("\n‚ö†Ô∏è  BASELINE COMPARISON ENABLED")
        print("  ‚Ä¢ Will compare compressed vs uncompressed outputs")
        print("  ‚Ä¢ Provides compression quality metrics")
        print("  ‚Ä¢ Evaluation time: ~2x longer")
        print("  ‚Ä¢ Recommended for research/publication")
    else:
        print("\n‚ö° FAST MODE (no baseline)")
        print("  ‚Ä¢ Evaluates compressed output only")
        print("  ‚Ä¢ Faster evaluation")
        print("  ‚Ä¢ Good for quick testing")

    # Run benchmark
    print("\n" + "=" * 80)
    print("STARTING BENCHMARK EVALUATION")
    print("=" * 80)
    
    benchmark.run_benchmark(
        dataset_names=[
            "squad",           # Extractive QA
            "hotpot_qa",       # Multi-hop reasoning
            "natural_questions",  # Open domain QA
            "coqa",            # Conversational QA
            "narrativeqa",     # Reading comprehension
        ],
        max_samples=5,  # Samples per dataset (increase for full eval)
        max_new_tokens=50,  # Max tokens to generate
    )

    print("\n" + "=" * 80)
    print("‚úÖ BENCHMARK COMPLETE")
    print("=" * 80)
    print("\nüìÅ Results saved to:")
    print("  ‚Ä¢ benchmark_results/benchmark_results_YYYYMMDD_HHMMSS.json")
    print("  ‚Ä¢ benchmark_results/benchmark_results_YYYYMMDD_HHMMSS.csv")
    print("\nüìä To visualize results, run:")
    print("  python visualize_benchmark.py")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()