"""
Advanced Compressed Vector Language (ACVL) - Research-Grade Implementation

FULLY CORRECTED VERSION:
- True Huffman entropy coding (variable-length codes)
- Adaptive PQ subvectors (dimension-aware)
- Proper quality-size tradeoff
"""

import json
import math
import struct
import warnings
import pickle
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union
from collections import Counter
import heapq

import numpy as np
from scipy.spatial.distance import cosine
from scipy.stats import spearmanr
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.cluster import MiniBatchKMeans
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


class CompressionLevel(Enum):
    """Compression levels with actual bit budgets"""
    MAX_FIDELITY = "max_fidelity"
    HIGH_FIDELITY = "high"
    BALANCED = "balanced"
    AGGRESSIVE = "aggressive"


class SemanticType(Enum):
    """Hierarchical semantic categorization"""
    ACTION = "action"
    SPATIAL = "spatial"
    TEMPORAL = "temporal"
    STATUS = "status"
    UNCERTAINTY = "uncertainty"


@dataclass
class CompressionConfig:
    """Configuration for research-grade compression"""
    max_embedding_dim: int = 384
    
    bit_budgets: Dict[str, int] = field(
        default_factory=lambda: {
            "max_fidelity": 128,
            "high": 64,
            "balanced": 32,
            "aggressive": 16,
        }
    )
    
    target_dims: Dict[str, int] = field(
        default_factory=lambda: {
            "max_fidelity": 64,
            "high": 32,
            "balanced": 16,
            "aggressive": 8,
        }
    )
    
    # ADAPTIVE: PQ subvectors now scale with dimension
    adaptive_pq_subvectors: Dict[str, int] = field(
        default_factory=lambda: {
            "max_fidelity": 16,  # 64D / 16 = 4D per subvector
            "high": 8,           # 32D / 8 = 4D per subvector
            "balanced": 8,       # 16D / 8 = 2D per subvector
            "aggressive": 4,     # 8D / 4 = 2D per subvector
        }
    )
    
    use_product_quantization: bool = True
    pq_bits_per_subvector: int = 8
    
    use_residual_coding: bool = True
    residual_stages: int = 2
    
    use_entropy_coding: bool = True  # TRUE Huffman coding now
    
    semantic_weight_lambda: float = 0.5
    
    semantic_weights: Dict[str, float] = field(
        default_factory=lambda: {
            "action": 1.0,
            "spatial": 0.9,
            "temporal": 0.8,
            "status": 0.6,
            "uncertainty": 0.4,
        }
    )


@dataclass
class CompressedVectorMessage:
    """Protocol-complete compressed message"""
    
    protocol_version: int = 1
    semantic_type_id: int = 0
    message_type_id: int = 0
    priority_level: int = 0
    compression_level: CompressionLevel = CompressionLevel.BALANCED
    timestamp: float = 0.0
    
    compressed_payload: bytes = b''
    
    codebook_version: int = 0
    projection_id: int = 0
    
    original_dim: int = 384
    compressed_bits: int = 32
    actual_compression_ratio: float = 1.0
    
    semantic_similarity_preserved: float = 0.0
    task_accuracy_preserved: float = 0.0
    reconstruction_rmse: float = 0.0
    
    checksum: int = 0
    
    def to_bytes(self) -> bytes:
        """Serialize with proper versioning and checksums"""
        header = struct.pack(
            "!HBBBBHH f I I H H f f f I",
            self.protocol_version,
            self.semantic_type_id,
            self.message_type_id,
            self.priority_level,
            list(CompressionLevel).index(self.compression_level),
            self.codebook_version,
            self.projection_id,
            self.timestamp,
            self.original_dim,
            self.compressed_bits,
            len(self.compressed_payload),
            0,
            self.actual_compression_ratio,
            self.semantic_similarity_preserved,
            self.reconstruction_rmse,
            self.checksum,
        )
        
        return header + self.compressed_payload
    
    @classmethod
    def from_bytes(cls, data: bytes) -> 'CompressedVectorMessage':
        """Deserialize with validation"""
        if len(data) < 32:
            raise ValueError("Invalid compressed message: too short")
        
        header = struct.unpack("!HBBBBHH f I I H H f f f I", data[:32])
        payload = data[32:32+header[10]]
        
        computed_checksum = hash(payload) % (2**32)
        if computed_checksum != header[15]:
            warnings.warn("Checksum mismatch - data may be corrupted")
        
        return cls(
            protocol_version=header[0],
            semantic_type_id=header[1],
            message_type_id=header[2],
            priority_level=header[3],
            compression_level=list(CompressionLevel)[header[4]],
            codebook_version=header[5],
            projection_id=header[6],
            timestamp=header[7],
            original_dim=header[8],
            compressed_bits=header[9],
            compressed_payload=payload,
            actual_compression_ratio=header[12],
            semantic_similarity_preserved=header[13],
            reconstruction_rmse=header[14],
            checksum=header[15],
        )


class HuffmanNode:
    """Node for Huffman tree"""
    def __init__(self, symbol=None, freq=0, left=None, right=None):
        self.symbol = symbol
        self.freq = freq
        self.left = left
        self.right = right
    
    def __lt__(self, other):
        return self.freq < other.freq


class HuffmanEncoder:
    """TRUE Huffman entropy coding with variable-length codes"""
    
    def __init__(self):
        self.symbol_counts = Counter()
        self.total_count = 0
        self.codebook = {}  # symbol -> bitstring
        self.reverse_codebook = {}  # bitstring -> symbol
        self.max_symbol = 0
        
    def fit(self, symbols: List[int]):
        """Build Huffman tree and codebook from symbol frequencies"""
        if not symbols:
            return
        
        self.symbol_counts = Counter(symbols)
        self.total_count = len(symbols)
        self.max_symbol = max(symbols)
        
        # Build Huffman tree
        if len(self.symbol_counts) == 1:
            # Special case: only one unique symbol
            symbol = list(self.symbol_counts.keys())[0]
            self.codebook = {symbol: '0'}
            self.reverse_codebook = {'0': symbol}
            return
        
        # Create leaf nodes
        heap = [HuffmanNode(symbol=sym, freq=count) 
                for sym, count in self.symbol_counts.items()]
        heapq.heapify(heap)
        
        # Build tree
        while len(heap) > 1:
            left = heapq.heappop(heap)
            right = heapq.heappop(heap)
            
            parent = HuffmanNode(
                symbol=None,
                freq=left.freq + right.freq,
                left=left,
                right=right
            )
            heapq.heappush(heap, parent)
        
        # Generate codes
        root = heap[0]
        self._generate_codes(root, "")
        
        # Build reverse mapping
        self.reverse_codebook = {code: sym for sym, code in self.codebook.items()}
    
    def _generate_codes(self, node, code):
        """Recursively generate Huffman codes"""
        if node is None:
            return
        
        if node.symbol is not None:
            # Leaf node
            self.codebook[node.symbol] = code if code else '0'
        else:
            # Internal node
            self._generate_codes(node.left, code + '0')
            self._generate_codes(node.right, code + '1')
    
    def encode(self, symbols: List[int]) -> bytes:
        """Encode symbols using Huffman codes"""
        if not self.codebook:
            # Fallback to fixed-length
            return np.array(symbols, dtype=np.uint8).tobytes()
        
        # Build bitstring
        bitstring = ""
        for sym in symbols:
            if sym in self.codebook:
                bitstring += self.codebook[sym]
            else:
                # Fallback: use code for most common symbol
                most_common = self.symbol_counts.most_common(1)[0][0]
                bitstring += self.codebook.get(most_common, '0')
        
        # Pad to byte boundary
        padding = (8 - len(bitstring) % 8) % 8
        bitstring += "0" * padding
        
        # Convert to bytes
        byte_array = bytearray()
        for i in range(0, len(bitstring), 8):
            byte = int(bitstring[i:i+8], 2)
            byte_array.append(byte)
        
        return bytes(byte_array)
    
    def decode(self, data: bytes, num_symbols: int) -> List[int]:
        """Decode Huffman-encoded data"""
        if not self.reverse_codebook:
            # Fallback to fixed-length
            return list(np.frombuffer(data, dtype=np.uint8)[:num_symbols])
        
        bitstring = "".join(format(byte, '08b') for byte in data)
        
        symbols = []
        current_code = ""
        
        for bit in bitstring:
            current_code += bit
            
            if current_code in self.reverse_codebook:
                symbols.append(self.reverse_codebook[current_code])
                current_code = ""
                
                if len(symbols) >= num_symbols:
                    break
        
        # Pad with most common symbol if needed
        if len(symbols) < num_symbols and self.symbol_counts:
            most_common = self.symbol_counts.most_common(1)[0][0]
            symbols.extend([most_common] * (num_symbols - len(symbols)))
        
        return symbols[:num_symbols]


class SemanticRateDistortionProjection:
    """PCA-based projection with configurable target dimensions"""
    
    def __init__(self, config: CompressionConfig, semantic_type: str):
        self.config = config
        self.semantic_type = semantic_type
        self.projection_matrix = None
        self.inverse_projection = None
        self.pca = None
        
    def fit(self, embeddings: np.ndarray, labels: Optional[np.ndarray] = None, 
            level_name: str = "balanced"):
        """Learn projection based on compression level's target dimensions"""
        
        n_samples, n_features = embeddings.shape
        
        target_dims = self.config.target_dims.get(level_name, 16)
        
        max_components = min(n_samples, n_features)
        target_dims = min(target_dims, max_components)
        
        if n_samples < 5 or target_dims < 2:
            print(f"    Warning: Too few samples ({n_samples}) or dims ({target_dims}), using identity")
            self.projection_matrix = np.eye(n_features)
            self.inverse_projection = np.eye(n_features)
            return self
        
        print(f"    Learning {target_dims}D projection from {n_features}D ({n_samples} samples)")
        
        self.pca = PCA(n_components=target_dims, random_state=42)
        self.pca.fit(embeddings)
        
        if labels is not None and len(labels) == len(embeddings):
            projected = self.pca.transform(embeddings)
            
            unique_labels = np.unique(labels)
            if len(unique_labels) > 1 and n_samples > len(unique_labels) * 2:
                try:
                    clf = LogisticRegression(max_iter=100, random_state=42)
                    clf.fit(projected, labels)
                    
                    score = clf.score(projected, labels)
                    print(f"    Semantic preservation in {target_dims}D: {score:.3f}")
                except Exception as e:
                    print(f"    Could not validate projection: {e}")
        
        self.projection_matrix = self.pca.components_
        self.inverse_projection = np.linalg.pinv(self.projection_matrix)
        
        return self
    
    def project(self, embedding: np.ndarray) -> np.ndarray:
        """Project to low dimensions"""
        if self.pca is None:
            return embedding
        
        try:
            return self.pca.transform(embedding.reshape(1, -1))[0]
        except Exception:
            return embedding
    
    def reconstruct(self, projected: np.ndarray) -> np.ndarray:
        """Reconstruct from projection"""
        if self.pca is None:
            return projected
        
        try:
            return self.pca.inverse_transform(projected.reshape(1, -1))[0]
        except Exception:
            return projected


class ProductQuantizer:
    """Product Quantization with adaptive subvectors"""
    
    def __init__(self, n_subvectors: int = 8, n_centroids: int = 256):
        self.n_subvectors = n_subvectors
        self.n_centroids = n_centroids
        self.codebooks = []
        self.subvector_dim = 0
        
    def fit(self, vectors: np.ndarray):
        """Train codebooks for each subvector"""
        n_samples, dim = vectors.shape
        
        if n_samples < 10:
            print(f"    Warning: Only {n_samples} samples for PQ, skipping")
            return
        
        self.subvector_dim = max(1, dim // self.n_subvectors)
        
        if self.subvector_dim == 0:
            self.subvector_dim = 1
            self.n_subvectors = dim
        
        print(f"    Training PQ: {self.n_subvectors} subvecs √ó {self.n_centroids} centroids "
              f"(subvec_dim={self.subvector_dim}, {n_samples} samples)")
        
        for i in range(self.n_subvectors):
            start_idx = i * self.subvector_dim
            end_idx = min(start_idx + self.subvector_dim, dim)
            
            subvectors = vectors[:, start_idx:end_idx]
            
            n_clusters = min(self.n_centroids, n_samples, max(2, len(subvectors)))
            
            if n_clusters < 2:
                dummy = type('obj', (object,), {'cluster_centers_': np.zeros((1, subvectors.shape[1]))})()
                self.codebooks.append(dummy)
                continue
            
            try:
                kmeans = MiniBatchKMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    n_init=3,
                    max_iter=100,
                    batch_size=min(100, n_samples)
                )
                kmeans.fit(subvectors)
                self.codebooks.append(kmeans)
            except Exception as e:
                print(f"    Warning: PQ failed for subvector {i}: {e}")
                dummy = type('obj', (object,), {'cluster_centers_': np.zeros((1, subvectors.shape[1]))})()
                self.codebooks.append(dummy)
    
    def encode(self, vector: np.ndarray) -> Tuple[List[int], np.ndarray]:
        """Encode vector as PQ indices"""
        indices = []
        reconstructed_parts = []
        
        dim = len(vector)
        
        for i, codebook in enumerate(self.codebooks):
            start_idx = i * self.subvector_dim
            end_idx = min(start_idx + self.subvector_dim, dim)
            
            subvec = vector[start_idx:end_idx]
            
            distances = np.linalg.norm(codebook.cluster_centers_ - subvec.reshape(1, -1), axis=1)
            idx = np.argmin(distances)
            
            indices.append(int(idx))
            reconstructed_parts.append(codebook.cluster_centers_[idx])
        
        reconstructed = np.concatenate(reconstructed_parts)
        
        if len(reconstructed) < dim:
            reconstructed = np.pad(reconstructed, (0, dim - len(reconstructed)))
        elif len(reconstructed) > dim:
            reconstructed = reconstructed[:dim]
        
        return indices, reconstructed
    
    def decode(self, indices: List[int]) -> np.ndarray:
        """Decode from PQ indices"""
        parts = []
        
        for idx, codebook in zip(indices, self.codebooks):
            idx = min(idx, len(codebook.cluster_centers_) - 1)
            parts.append(codebook.cluster_centers_[idx])
        
        return np.concatenate(parts)


class ResidualVectorQuantizer:
    """Multi-stage residual quantization"""
    
    def __init__(self, n_stages: int = 2, n_centroids: int = 256):
        self.n_stages = n_stages
        self.n_centroids = n_centroids
        self.codebooks = []
        
    def fit(self, vectors: np.ndarray):
        """Train residual codebooks"""
        n_samples = len(vectors)
        
        if n_samples < 10:
            print(f"    Warning: Only {n_samples} samples for RVQ, skipping")
            return
        
        print(f"    Training RVQ: {self.n_stages} stages √ó {self.n_centroids} centroids ({n_samples} samples)")
        
        residual = vectors.copy()
        
        for stage in range(self.n_stages):
            n_clusters = min(self.n_centroids, n_samples, max(2, len(residual)))
            
            if n_clusters < 2:
                print(f"    Warning: Skipping RVQ stage {stage}")
                break
            
            try:
                kmeans = MiniBatchKMeans(
                    n_clusters=n_clusters,
                    random_state=42 + stage,
                    n_init=3,
                    max_iter=50,
                    batch_size=min(100, n_samples)
                )
                kmeans.fit(residual)
                
                self.codebooks.append(kmeans)
                
                predictions = kmeans.predict(residual)
                quantized = kmeans.cluster_centers_[predictions]
                residual = residual - quantized
            
            except Exception as e:
                print(f"    Warning: RVQ failed at stage {stage}: {e}")
                break
    
    def encode(self, vector: np.ndarray) -> Tuple[List[int], np.ndarray]:
        """Encode with residual stages"""
        indices = []
        reconstructed = np.zeros_like(vector)
        residual = vector.copy()
        
        for codebook in self.codebooks:
            distances = np.linalg.norm(codebook.cluster_centers_ - residual.reshape(1, -1), axis=1)
            idx = np.argmin(distances)
            
            indices.append(int(idx))
            
            quantized = codebook.cluster_centers_[idx]
            reconstructed += quantized
            residual = residual - quantized
        
        return indices, reconstructed
    
    def decode(self, indices: List[int]) -> np.ndarray:
        """Decode from residual indices"""
        reconstructed = None
        
        for idx, codebook in zip(indices, self.codebooks):
            idx = min(idx, len(codebook.cluster_centers_) - 1)
            
            if reconstructed is None:
                reconstructed = codebook.cluster_centers_[idx].copy()
            else:
                reconstructed += codebook.cluster_centers_[idx]
        
        return reconstructed if reconstructed is not None else np.zeros(1)


class SemanticClassifier:
    """Validated semantic type classifier"""
    
    def __init__(self):
        self.classifier = None
        self.label_encoder = {}
        self.accuracy = 0.0
        
    def fit(self, embeddings: np.ndarray, messages: List[Dict]):
        """Train classifier with validation"""
        labels = [self._heuristic_classify(msg) for msg in messages]
        
        unique_labels = list(set(labels))
        self.label_encoder = {label: i for i, label in enumerate(unique_labels)}
        
        y = np.array([self.label_encoder[label] for label in labels])
        
        n_train = int(0.8 * len(embeddings))
        X_train, X_val = embeddings[:n_train], embeddings[n_train:]
        y_train, y_val = y[:n_train], y[n_train:]
        
        self.classifier = LogisticRegression(max_iter=200, random_state=42)
        self.classifier.fit(X_train, y_train)
        
        y_pred = self.classifier.predict(X_val)
        self.accuracy = accuracy_score(y_val, y_pred)
        
        print(f"  Semantic classifier accuracy: {self.accuracy:.3f}")
        
        return labels
    
    def _heuristic_classify(self, message: Dict) -> str:
        """Heuristic classification"""
        content = message["content"].lower()
        msg_type = message["message_type"].lower()
        
        action_terms = ["move", "navigate", "execute", "abort", "land", "takeoff", "proceed"]
        spatial_terms = ["coordinates", "bearing", "altitude", "position", "location", "waypoint"]
        temporal_terms = ["emergency", "immediate", "urgent", "time", "eta", "deadline"]
        
        if any(term in content for term in action_terms) or "navigation" in msg_type:
            return SemanticType.ACTION.value
        elif any(term in content for term in spatial_terms):
            return SemanticType.SPATIAL.value
        elif any(term in content for term in temporal_terms) or "emergency" in msg_type:
            return SemanticType.TEMPORAL.value
        elif "status" in msg_type or "diagnostics" in content:
            return SemanticType.STATUS.value
        else:
            return SemanticType.UNCERTAINTY.value
    
    def classify(self, embedding: np.ndarray) -> str:
        """Classify with learned model"""
        if self.classifier is None:
            return SemanticType.ACTION.value
        
        y_pred = self.classifier.predict(embedding.reshape(1, -1))[0]
        
        reverse_encoder = {v: k for k, v in self.label_encoder.items()}
        return reverse_encoder.get(y_pred, SemanticType.ACTION.value)


class AdvancedCVL:
    """Research-grade compressed vector language (FULLY CORRECTED)"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", 
                 config: Optional[CompressionConfig] = None):
        self.config = config or CompressionConfig()
        
        print("Loading sentence transformer model...")
        self.sentence_model = SentenceTransformer(model_name)
        
        self.semantic_classifier = SemanticClassifier()
        self.projections = {}
        self.product_quantizers = {}
        self.residual_quantizers = {}
        self.huffman_encoders = {}
        
        self.message_type_vocab = {}
        self.priority_vocab = {}
        
        self.codebook_version = 1
        
        self.is_trained = False
        
        print("üî¨ Research-Grade CVL System Initialized (FULLY CORRECTED)")
        print(f"‚úì PCA-based projections (level-specific dimensions)")
        print(f"‚úì Adaptive Product Quantization (dimension-aware subvectors)")
        print(f"‚úì Residual Vector Quantization (2 stages)")
        print(f"‚úì TRUE Huffman entropy coding (variable-length codes)")
        print(f"‚úì Validated semantic preservation metrics")
    
    def fit(self, messages: List[Dict[str, Any]], validation_split: float = 0.2):
        """Train with validation and metrics"""
        print("\nüî¨ Training Research-Grade Compression System...")
        
        unique_types = list(set(msg["message_type"] for msg in messages))
        unique_priorities = list(set(msg["priority"] for msg in messages))
        self.message_type_vocab = {t: i for i, t in enumerate(unique_types)}
        self.priority_vocab = {p: i for i, p in enumerate(unique_priorities)}
        
        contents = [msg["content"] for msg in messages]
        print("Generating semantic embeddings...")
        embeddings = self.sentence_model.encode(contents, show_progress_bar=True)
        
        print("Training semantic classifier...")
        semantic_labels = self.semantic_classifier.fit(embeddings, messages)
        
        label_to_id = {label: i for i, label in enumerate(set(semantic_labels))}
        numeric_labels = np.array([label_to_id[label] for label in semantic_labels])
        
        print("Learning level-specific projections with adaptive quantization...")
        
        for sem_type in set(semantic_labels):
            mask = np.array([label == sem_type for label in semantic_labels])
            type_embeddings = embeddings[mask]
            type_numeric_labels = numeric_labels[mask]
            
            if len(type_embeddings) < 10:
                print(f"\n  Semantic type: {sem_type} - SKIPPED (only {len(type_embeddings)} samples)")
                continue
            
            print(f"\n  Semantic type: {sem_type} ({len(type_embeddings)} samples)")
            
            for level_name in self.config.target_dims.keys():
                key = f"{sem_type}_{level_name}"
                
                # Create projection with level-specific dimensions
                projection = SemanticRateDistortionProjection(self.config, sem_type)
                projection.fit(type_embeddings, type_numeric_labels, level_name)
                
                self.projections[key] = projection
                
                # Get projected data
                projected = np.array([projection.project(emb) for emb in type_embeddings])
                
                # ADAPTIVE: Use level-specific number of subvectors
                n_subvectors = self.config.adaptive_pq_subvectors.get(level_name, 8)
                
                if self.config.use_product_quantization and len(projected) >= 10:
                    pq = ProductQuantizer(
                        n_subvectors=n_subvectors,
                        n_centroids=2 ** self.config.pq_bits_per_subvector
                    )
                    pq.fit(projected)
                    self.product_quantizers[key] = pq
                
                if self.config.use_residual_coding and len(projected) >= 10:
                    rvq = ResidualVectorQuantizer(
                        n_stages=self.config.residual_stages,
                        n_centroids=256
                    )
                    rvq.fit(projected)
                    self.residual_quantizers[key] = rvq
                
                # Train TRUE Huffman encoder
                if self.config.use_entropy_coding:
                    all_indices = []
                    
                    if self.config.use_product_quantization and key in self.product_quantizers:
                        for emb in projected[:100]:
                            try:
                                indices, _ = self.product_quantizers[key].encode(emb)
                                all_indices.extend(indices)
                            except:
                                pass
                    
                    if self.config.use_residual_coding and key in self.residual_quantizers:
                        for emb in projected[:100]:
                            try:
                                indices, _ = self.residual_quantizers[key].encode(emb)
                                all_indices.extend(indices)
                            except:
                                pass
                    
                    if all_indices:
                        encoder = HuffmanEncoder()
                        encoder.fit(all_indices)
                        self.huffman_encoders[key] = encoder
                        
                        # Report Huffman statistics
                        if encoder.codebook:
                            avg_code_len = np.mean([len(code) for code in encoder.codebook.values()])
                            print(f"    Huffman coding: avg {avg_code_len:.2f} bits/symbol "
                                  f"({len(encoder.codebook)} unique symbols)")
        
        self.is_trained = True
        print("\n‚úÖ Training complete with adaptive quantization and Huffman coding!")
        
        n_val = max(100, int(len(embeddings) * validation_split))
        val_embeddings = embeddings[-n_val:]
        val_messages = messages[-n_val:]
        
        print("\nüìä Validation Metrics:")
        self._compute_validation_metrics(val_embeddings, val_messages)
        
        return {
            "num_messages": len(messages),
            "embedding_dimension": embeddings.shape[1],
            "message_types": len(unique_types),
            "semantic_types": len(set(semantic_labels)),
            "semantic_classifier_accuracy": self.semantic_classifier.accuracy,
            "codebook_version": self.codebook_version,
        }
    
    def _compute_validation_metrics(self, embeddings: np.ndarray, messages: List[Dict]):
        """Compute rigorous validation metrics with debugging"""
        
        similarities_original = []
        similarities_reconstructed = []
        
        n_pairs = min(100, len(embeddings) - 1)
        
        compression_failures = 0
        
        for i in range(n_pairs):
            try:
                sim_orig = 1 - cosine(embeddings[i], embeddings[i+1])
                if not np.isnan(sim_orig):
                    similarities_original.append(sim_orig)
            except Exception as e:
                continue
            
            try:
                compressed = self.compress(messages[i], CompressionLevel.BALANCED)
                decompressed = self.decompress(compressed)
                
                reconstructed_emb = decompressed.get("reconstructed_embedding")
                
                if reconstructed_emb is not None and len(reconstructed_emb) == len(embeddings[i+1]):
                    sim_recon = 1 - cosine(reconstructed_emb, embeddings[i+1])
                    if not np.isnan(sim_recon):
                        similarities_reconstructed.append(sim_recon)
                    else:
                        compression_failures += 1
                else:
                    compression_failures += 1
            except Exception as e:
                compression_failures += 1
        
        if compression_failures > 0:
            print(f"  Compression failures: {compression_failures}/{n_pairs}")
        
        if len(similarities_original) > 10 and len(similarities_reconstructed) > 10:
            try:
                min_len = min(len(similarities_original), len(similarities_reconstructed))
                similarities_original = similarities_original[:min_len]
                similarities_reconstructed = similarities_reconstructed[:min_len]
                
                correlation, pvalue = spearmanr(similarities_original, similarities_reconstructed)
                print(f"  Pairwise similarity preservation (Spearman œÅ): {correlation:.3f} (p={pvalue:.4f})")
            except Exception as e:
                print(f"  Could not compute Spearman correlation: {e}")
        else:
            print(f"  Insufficient validation samples for correlation ({len(similarities_original)} pairs)")
    
    def compress(self, message: Dict[str, Any], 
                 compression_level: CompressionLevel = CompressionLevel.BALANCED) -> CompressedVectorMessage:
        """Compress with level-specific adaptive pipeline"""
        
        embedding = self.sentence_model.encode([message["content"]])[0]
        
        semantic_type = self.semantic_classifier.classify(embedding)
        
        key = f"{semantic_type}_{compression_level.value}"
        
        if key not in self.projections:
            if self.projections:
                key = list(self.projections.keys())[0]
            else:
                raise ValueError("No trained projections available")
        
        projection = self.projections[key]
        
        projected = projection.project(embedding)
        
        indices = []
        reconstructed = projected.copy()
        
        if self.config.use_product_quantization and key in self.product_quantizers:
            try:
                pq_indices, pq_reconstructed = self.product_quantizers[key].encode(projected)
                indices.extend(pq_indices)
                reconstructed = pq_reconstructed
                
                if self.config.use_residual_coding and key in self.residual_quantizers:
                    residual = projected - pq_reconstructed
                    rvq_indices, rvq_reconstructed = self.residual_quantizers[key].encode(residual)
                    indices.extend(rvq_indices)
                    reconstructed = pq_reconstructed + rvq_reconstructed
            except Exception as e:
                indices = [0] * 8
        
        if not indices:
            indices = [0] * 8
        
        # Use Huffman encoding
        if self.config.use_entropy_coding and key in self.huffman_encoders:
            try:
                compressed_payload = self.huffman_encoders[key].encode(indices)
            except:
                compressed_payload = np.array(indices, dtype=np.uint8).tobytes()
        else:
            compressed_payload = np.array(indices, dtype=np.uint8).tobytes()
        
        try:
            reconstructed_full = projection.reconstruct(reconstructed)
        except:
            reconstructed_full = embedding
        
        original_size = len(json.dumps(message).encode("utf-8"))
        compressed_size = len(compressed_payload) + 32
        compression_ratio = original_size / max(compressed_size, 1)
        
        try:
            semantic_similarity = 1 - cosine(embedding, reconstructed_full)
        except:
            semantic_similarity = 0.5
        
        reconstruction_rmse = np.sqrt(np.mean((embedding - reconstructed_full) ** 2))
        
        checksum = hash(compressed_payload.tobytes() if isinstance(compressed_payload, np.ndarray) else compressed_payload) % (2**32)
        
        return CompressedVectorMessage(
            protocol_version=1,
            semantic_type_id=list(SemanticType).index(SemanticType(semantic_type)),
            message_type_id=self.message_type_vocab.get(message["message_type"], 0),
            priority_level=self.priority_vocab.get(message["priority"], 0),
            compression_level=compression_level,
            timestamp=message.get("timestamp", 0.0),
            compressed_payload=compressed_payload,
            codebook_version=self.codebook_version,
            projection_id=hash(key) % (2**16),
            original_dim=len(embedding),
            compressed_bits=len(compressed_payload) * 8,
            actual_compression_ratio=compression_ratio,
            semantic_similarity_preserved=float(np.clip(semantic_similarity, 0, 1)),
            task_accuracy_preserved=0.0,
            reconstruction_rmse=float(reconstruction_rmse),
            checksum=checksum,
        )
    
    def decompress(self, compressed_msg: CompressedVectorMessage) -> Dict[str, Any]:
        """Decompress with validation"""
        
        computed_checksum = hash(compressed_msg.compressed_payload) % (2**32)
        if computed_checksum != compressed_msg.checksum:
            warnings.warn("Checksum mismatch - data may be corrupted")
        
        semantic_type = list(SemanticType)[compressed_msg.semantic_type_id].value
        key = f"{semantic_type}_{compressed_msg.compression_level.value}"
        
        if key not in self.projections:
            if self.projections:
                key = list(self.projections.keys())[0]
            else:
                raise ValueError("No projections available for decompression")
        
        projection = self.projections[key]
        
        # Determine number of indices
        n_subvectors = self.config.adaptive_pq_subvectors.get(
            compressed_msg.compression_level.value, 8
        )
        num_indices = n_subvectors
        if self.config.use_residual_coding:
            num_indices += self.config.residual_stages
        
        # Decode with Huffman
        if self.config.use_entropy_coding and key in self.huffman_encoders:
            try:
                indices = self.huffman_encoders[key].decode(
                    compressed_msg.compressed_payload, 
                    num_indices
                )
            except:
                indices = list(np.frombuffer(compressed_msg.compressed_payload, dtype=np.uint8)[:num_indices])
        else:
            indices = list(np.frombuffer(compressed_msg.compressed_payload, dtype=np.uint8)[:num_indices])
        
        try:
            if self.config.use_product_quantization and key in self.product_quantizers:
                pq_indices = indices[:n_subvectors]
                reconstructed = self.product_quantizers[key].decode(pq_indices)
                
                if self.config.use_residual_coding and key in self.residual_quantizers:
                    rvq_indices = indices[n_subvectors:]
                    residual = self.residual_quantizers[key].decode(rvq_indices)
                    reconstructed = reconstructed + residual
            else:
                reconstructed = np.zeros(compressed_msg.original_dim)
        except:
            reconstructed = np.zeros(compressed_msg.original_dim)
        
        try:
            reconstructed_embedding = projection.reconstruct(reconstructed)
        except:
            reconstructed_embedding = np.zeros(compressed_msg.original_dim)
        
        reverse_type_vocab = {v: k for k, v in self.message_type_vocab.items()}
        reverse_priority_vocab = {v: k for k, v in self.priority_vocab.items()}
        
        return {
            "message_type": reverse_type_vocab.get(compressed_msg.message_type_id, "unknown"),
            "priority": reverse_priority_vocab.get(compressed_msg.priority_level, "normal"),
            "semantic_type": semantic_type,
            "timestamp": compressed_msg.timestamp,
            "compression_level": compressed_msg.compression_level.value,
            "compression_ratio": compressed_msg.actual_compression_ratio,
            "semantic_similarity_preserved": compressed_msg.semantic_similarity_preserved,
            "reconstruction_rmse": compressed_msg.reconstruction_rmse,
            "reconstructed_embedding": reconstructed_embedding,
        }
    
    def export_codebooks(self, path: str):
        """Export codebooks for distributed deployment"""
        codebooks = {
            "version": self.codebook_version,
            "projections": {k: {
                "pca_components": v.pca.components_ if v.pca else None,
                "pca_mean": v.pca.mean_ if v.pca else None,
            } for k, v in self.projections.items()},
            "product_quantizers": {k: {
                "codebooks": [cb.cluster_centers_ for cb in v.codebooks]
            } for k, v in self.product_quantizers.items()},
            "huffman_encoders": {k: {
                "codebook": v.codebook,
                "symbol_counts": dict(v.symbol_counts),
            } for k, v in self.huffman_encoders.items()},
            "message_type_vocab": self.message_type_vocab,
            "priority_vocab": self.priority_vocab,
        }
        
        with open(path, 'wb') as f:
            pickle.dump(codebooks, f)
        
        print(f"‚úÖ Codebooks exported to {path}")


def main():
    """Research-grade demonstration (FULLY CORRECTED)"""
    from real_data_generator import RealAgentDataGenerator
    
    generator = RealAgentDataGenerator()
    messages = generator.generate_dataset(1000)
    
    print(f"Generated {len(messages)} test messages\n")
    
    config = CompressionConfig()
    
    acvl = AdvancedCVL(config=config)
    training_stats = acvl.fit(messages, validation_split=0.2)
    
    print(f"\nüìä Training Stats: {training_stats}")
    
    test_msg = messages[0]
    print(f"\nüìù Test Message: {test_msg['content'][:100]}...")
    
    print("\n" + "="*60)
    print("COMPRESSION LEVELS TEST")
    print("="*60)
    
    for level in CompressionLevel:
        try:
            compressed = acvl.compress(test_msg, level)
            decompressed = acvl.decompress(compressed)
            
            print(f"\n{level.value.upper()}:")
            print(f"  Ratio: {compressed.actual_compression_ratio:.2f}x")
            print(f"  Size: {len(compressed.to_bytes())} bytes ({compressed.compressed_bits} bits payload)")
            print(f"  Semantic similarity: {compressed.semantic_similarity_preserved:.3f}")
            print(f"  RMSE: {compressed.reconstruction_rmse:.4f}")
        except Exception as e:
            print(f"\n{level.value.upper()}: FAILED - {e}")
    
    acvl.export_codebooks("codebooks_v1.pkl")


if __name__ == "__main__":
    main()